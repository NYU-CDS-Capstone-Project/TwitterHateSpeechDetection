{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Import libraries<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import model_selection\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drew inspiration from\n",
    "#https://github.com/dmesquita/understanding_pytorch_nn and\n",
    "#https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "#https://github.com/nyu-mll/DS-GA-1011-Fall2017/blob/master/week%20eight/Week%20Eight%20Solutions.ipynb\n",
    "#https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "#https://github.com/claravania/lstm-pytorch/blob/master/model.py\n",
    "#https://medium.com/@sonicboom8/sentiment-analysis-with-variable-length-sequences-in-pytorch-6241635ae130\n",
    "#https://github.com/hpanwar08/sentence-classification-pytorch/blob/master/Sentiment%20analysis%20pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://medium.com/@martinpella/how-to-use-pre-transfered-word-embeddings-in-pytorch-71ca59249f76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://modelzoo.co/model/pytorch-nlp\n",
    "#http://anie.me/On-Torchtext/\n",
    "#https://readthedocs.org/projects/pytorchnlp/downloads/pdf/latest/\n",
    "#https://github.com/A-Jacobson/CNN_Sentence_Classification/blob/master/WordVectors.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Data Processing<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transfer = pd.read_csv(\"../clean_tl_data_nn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../train_nn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.loc[train['clean_tweet'].isnull() == False,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"its cool its str8 i know y'all got the wrong bitch i understand !\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer['clean_tweet'][12000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize(x):\n",
    "    if x == 0:\n",
    "        return (1)\n",
    "    else:\n",
    "        return (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transfer['hatespeech'] = [categorize(x) for x in transfer['class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23353\n",
       "1     1430\n",
       "Name: hatespeech, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer['hatespeech'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=24783, step=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transfer.reset_index(inplace = True, drop = True)\n",
    "\n",
    "all_clean_tweets = pd.concat([train.clean_tweet, transfer.clean_tweet])\n",
    "\n",
    "all_clean_tweets.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in all_clean_tweets:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_words = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'most_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-83d6fd487eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'most_common'"
     ]
    }
   ],
   "source": [
    "vocab = dict(vocab.most_common(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "index2word = {}\n",
    "\n",
    "word2index['PAD'] = 0\n",
    "word2index['UNK'] = 1\n",
    "index2word[0] = 'PAD'\n",
    "index2word[1] = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,word in enumerate(vocab):\n",
    "    word2index[word.lower()] = i+2\n",
    "    index2word[i+2] = word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'word2index.pkl'\n",
    "pickle.dump(word2index, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'index2word.pkl'\n",
    "pickle.dump(index2word, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_glove(path):\n",
    "    \"\"\"\n",
    "    creates a dictionary mapping words to vectors from a file in glove format.\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        glove = {}\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            glove[word] = vector\n",
    "        return glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 15s, sys: 14.1 s, total: 1min 29s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"/Users/carolineroper/Desktop/Capstone Project/Neural Network/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "%time glove = load_glove(glove_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix_len = total_words\n",
    "weights_matrix = np.zeros((matrix_len, 200))\n",
    "words_found = 0\n",
    "\n",
    "for i in range(0, total_words):\n",
    "    try: \n",
    "        weights_matrix[i] = glove[index2word[i]]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.rand(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73997600239976"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_found/total_words #74% of words were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_matrix[0] = np.zeros(200) #initialize pad embedding to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.46830010e-01,  -1.96119994e-01,  -3.49229991e-01,\n",
       "        -2.81580001e-01,  -7.56269991e-01,  -4.00349982e-02,\n",
       "         5.34219980e-01,   1.53270003e-03,  -2.19630003e-01,\n",
       "        -5.67080021e-01,  -7.51120001e-02,   3.90740007e-01,\n",
       "         1.92010000e-01,   4.80460003e-02,  -1.68009996e-01,\n",
       "        -1.91400006e-01,   1.21619999e-01,  -2.25130007e-01,\n",
       "         2.22759992e-02,  -2.76320010e-01,   1.07210003e-01,\n",
       "        -5.81910014e-02,  -1.76540002e-01,  -2.06199996e-02,\n",
       "        -3.97679992e-02,   1.26190007e-01,   1.89270005e-01,\n",
       "         1.70169994e-01,  -2.34529991e-02,  -4.23489988e-01,\n",
       "        -4.26400006e-02,  -2.81010002e-01,  -3.24609995e-01,\n",
       "         3.08699995e-01,   9.45290029e-02,   1.35590002e-01,\n",
       "        -5.02489984e-01,   3.00720006e-01,   1.58050001e-01,\n",
       "         5.50790012e-01,  -3.70050013e-01,  -2.17209995e-01,\n",
       "        -7.11619973e-01,   4.29749995e-01,  -1.24509996e-02,\n",
       "        -2.42750004e-01,  -6.29020035e-02,   4.37549986e-02,\n",
       "         5.90980016e-02,   2.15529993e-01,   3.40479985e-02,\n",
       "        -1.57350004e-01,  -4.47309986e-02,  -1.27189994e-01,\n",
       "         3.33469987e-01,   2.23859996e-01,   3.97159994e-01,\n",
       "         8.43819976e-02,  -4.70569991e-02,  -1.49430007e-01,\n",
       "         2.01399997e-02,  -5.13449982e-02,  -1.77820008e-02,\n",
       "        -4.85579997e-01,  -4.40770015e-02,   3.86900008e-01,\n",
       "        -3.51390004e-01,   8.89970005e-01,   6.69700027e-01,\n",
       "        -4.40119989e-02,   4.26730007e-01,  -1.96710005e-01,\n",
       "        -5.85529990e-02,   1.02069996e-01,  -3.70260000e-01,\n",
       "         2.96330005e-01,   4.60469991e-01,   3.56990010e-01,\n",
       "        -2.15639994e-01,   5.06760001e-01,   4.05409992e-01,\n",
       "         4.15380001e-01,   5.34810007e-01,   2.20500007e-01,\n",
       "         1.55780002e-01,  -5.70949972e-01,  -5.50019979e-01,\n",
       "         5.38770020e-01,   3.34190011e-01,  -3.31999987e-01,\n",
       "        -2.02110007e-01,  -3.72189999e-01,  -1.10299997e-01,\n",
       "         8.95290017e-01,  -2.10519999e-01,  -1.30119994e-01,\n",
       "        -2.42339998e-01,  -3.03470008e-02,   2.25569993e-01,\n",
       "         2.46030003e-01,  -4.70919997e-01,   6.57190010e-02,\n",
       "        -7.65509978e-02,  -2.37489998e-01,  -2.78149992e-01,\n",
       "         2.20500007e-01,   2.05669999e-01,   5.34839988e-01,\n",
       "        -1.17660001e-01,   8.30340013e-02,  -5.71230017e-02,\n",
       "        -1.76139995e-01,  -4.97150004e-01,   1.28289998e-01,\n",
       "        -1.52419999e-01,  -7.73880005e-01,  -7.81400025e-01,\n",
       "        -4.31719989e-01,   6.76060021e-01,   2.92690009e-01,\n",
       "         1.96710005e-01,   5.05530000e-01,  -1.89209998e-01,\n",
       "        -1.88999996e-01,   7.10150003e-02,  -3.93469989e-01,\n",
       "         7.74319982e-03,  -7.63300002e-01,  -4.18280005e-01,\n",
       "         4.38820004e-01,   8.99469972e-01,  -2.40669996e-01,\n",
       "         1.38630003e-01,   2.53309995e-01,  -1.08690001e-02,\n",
       "        -1.01340003e-01,  -3.43650013e-01,   7.19609976e-01,\n",
       "         1.68559998e-01,   9.60540026e-02,  -1.72350004e-01,\n",
       "        -5.26499987e-01,   1.96500003e-01,  -9.11900029e-02,\n",
       "        -1.76569998e-01,   1.48699999e-01,  -2.31759995e-02,\n",
       "         9.75740016e-01,   7.65380025e-01,  -2.87939996e-01,\n",
       "         3.57760012e-01,   1.43210003e-02,  -3.83780003e+00,\n",
       "        -1.78489998e-01,  -4.89069998e-01,   4.22560014e-02,\n",
       "        -6.94400012e-01,  -3.79290015e-01,  -4.33890000e-02,\n",
       "        -1.56560004e-01,   7.40360022e-01,  -3.70370001e-01,\n",
       "        -3.35020006e-01,  -5.39570004e-02,  -1.74779996e-01,\n",
       "        -6.73770010e-02,   4.20540005e-01,  -5.86589985e-02,\n",
       "        -2.42180005e-01,  -8.40779990e-02,  -3.03719997e-01,\n",
       "         1.35490000e-01,   2.70880014e-01,   4.79490012e-01,\n",
       "         3.33929993e-02,   7.09469974e-01,  -2.88120002e-01,\n",
       "         2.96270013e-01,  -4.10059988e-01,  -2.76690006e-01,\n",
       "        -1.70460001e-01,   3.84479985e-02,  -1.07420003e-02,\n",
       "         3.82499993e-01,   8.68320018e-02,  -1.78350005e-02,\n",
       "        -7.03899980e-01,   1.96139999e-02,   8.27580038e-03,\n",
       "         3.20300013e-01,   3.50510003e-03,   3.31299990e-01,\n",
       "         1.53259993e-01,  -2.20070004e-01,  -4.57010001e-01,\n",
       "        -1.77190006e-02,  -6.19970024e-01,  -5.20730019e-01,\n",
       "         8.22940022e-02,  -5.44780016e-01], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.46830010e-01,  -1.96119994e-01,  -3.49229991e-01,\n",
       "        -2.81580001e-01,  -7.56269991e-01,  -4.00349982e-02,\n",
       "         5.34219980e-01,   1.53270003e-03,  -2.19630003e-01,\n",
       "        -5.67080021e-01,  -7.51120001e-02,   3.90740007e-01,\n",
       "         1.92010000e-01,   4.80460003e-02,  -1.68009996e-01,\n",
       "        -1.91400006e-01,   1.21619999e-01,  -2.25130007e-01,\n",
       "         2.22759992e-02,  -2.76320010e-01,   1.07210003e-01,\n",
       "        -5.81910014e-02,  -1.76540002e-01,  -2.06199996e-02,\n",
       "        -3.97679992e-02,   1.26190007e-01,   1.89270005e-01,\n",
       "         1.70169994e-01,  -2.34529991e-02,  -4.23489988e-01,\n",
       "        -4.26400006e-02,  -2.81010002e-01,  -3.24609995e-01,\n",
       "         3.08699995e-01,   9.45290029e-02,   1.35590002e-01,\n",
       "        -5.02489984e-01,   3.00720006e-01,   1.58050001e-01,\n",
       "         5.50790012e-01,  -3.70050013e-01,  -2.17209995e-01,\n",
       "        -7.11619973e-01,   4.29749995e-01,  -1.24509996e-02,\n",
       "        -2.42750004e-01,  -6.29020035e-02,   4.37549986e-02,\n",
       "         5.90980016e-02,   2.15529993e-01,   3.40479985e-02,\n",
       "        -1.57350004e-01,  -4.47309986e-02,  -1.27189994e-01,\n",
       "         3.33469987e-01,   2.23859996e-01,   3.97159994e-01,\n",
       "         8.43819976e-02,  -4.70569991e-02,  -1.49430007e-01,\n",
       "         2.01399997e-02,  -5.13449982e-02,  -1.77820008e-02,\n",
       "        -4.85579997e-01,  -4.40770015e-02,   3.86900008e-01,\n",
       "        -3.51390004e-01,   8.89970005e-01,   6.69700027e-01,\n",
       "        -4.40119989e-02,   4.26730007e-01,  -1.96710005e-01,\n",
       "        -5.85529990e-02,   1.02069996e-01,  -3.70260000e-01,\n",
       "         2.96330005e-01,   4.60469991e-01,   3.56990010e-01,\n",
       "        -2.15639994e-01,   5.06760001e-01,   4.05409992e-01,\n",
       "         4.15380001e-01,   5.34810007e-01,   2.20500007e-01,\n",
       "         1.55780002e-01,  -5.70949972e-01,  -5.50019979e-01,\n",
       "         5.38770020e-01,   3.34190011e-01,  -3.31999987e-01,\n",
       "        -2.02110007e-01,  -3.72189999e-01,  -1.10299997e-01,\n",
       "         8.95290017e-01,  -2.10519999e-01,  -1.30119994e-01,\n",
       "        -2.42339998e-01,  -3.03470008e-02,   2.25569993e-01,\n",
       "         2.46030003e-01,  -4.70919997e-01,   6.57190010e-02,\n",
       "        -7.65509978e-02,  -2.37489998e-01,  -2.78149992e-01,\n",
       "         2.20500007e-01,   2.05669999e-01,   5.34839988e-01,\n",
       "        -1.17660001e-01,   8.30340013e-02,  -5.71230017e-02,\n",
       "        -1.76139995e-01,  -4.97150004e-01,   1.28289998e-01,\n",
       "        -1.52419999e-01,  -7.73880005e-01,  -7.81400025e-01,\n",
       "        -4.31719989e-01,   6.76060021e-01,   2.92690009e-01,\n",
       "         1.96710005e-01,   5.05530000e-01,  -1.89209998e-01,\n",
       "        -1.88999996e-01,   7.10150003e-02,  -3.93469989e-01,\n",
       "         7.74319982e-03,  -7.63300002e-01,  -4.18280005e-01,\n",
       "         4.38820004e-01,   8.99469972e-01,  -2.40669996e-01,\n",
       "         1.38630003e-01,   2.53309995e-01,  -1.08690001e-02,\n",
       "        -1.01340003e-01,  -3.43650013e-01,   7.19609976e-01,\n",
       "         1.68559998e-01,   9.60540026e-02,  -1.72350004e-01,\n",
       "        -5.26499987e-01,   1.96500003e-01,  -9.11900029e-02,\n",
       "        -1.76569998e-01,   1.48699999e-01,  -2.31759995e-02,\n",
       "         9.75740016e-01,   7.65380025e-01,  -2.87939996e-01,\n",
       "         3.57760012e-01,   1.43210003e-02,  -3.83780003e+00,\n",
       "        -1.78489998e-01,  -4.89069998e-01,   4.22560014e-02,\n",
       "        -6.94400012e-01,  -3.79290015e-01,  -4.33890000e-02,\n",
       "        -1.56560004e-01,   7.40360022e-01,  -3.70370001e-01,\n",
       "        -3.35020006e-01,  -5.39570004e-02,  -1.74779996e-01,\n",
       "        -6.73770010e-02,   4.20540005e-01,  -5.86589985e-02,\n",
       "        -2.42180005e-01,  -8.40779990e-02,  -3.03719997e-01,\n",
       "         1.35490000e-01,   2.70880014e-01,   4.79490012e-01,\n",
       "         3.33929993e-02,   7.09469974e-01,  -2.88120002e-01,\n",
       "         2.96270013e-01,  -4.10059988e-01,  -2.76690006e-01,\n",
       "        -1.70460001e-01,   3.84479985e-02,  -1.07420003e-02,\n",
       "         3.82499993e-01,   8.68320018e-02,  -1.78350005e-02,\n",
       "        -7.03899980e-01,   1.96139999e-02,   8.27580038e-03,\n",
       "         3.20300013e-01,   3.50510003e-03,   3.31299990e-01,\n",
       "         1.53259993e-01,  -2.20070004e-01,  -4.57010001e-01,\n",
       "        -1.77190006e-02,  -6.19970024e-01,  -5.20730019e-01,\n",
       "         8.22940022e-02,  -5.44780016e-01])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix[1797, ] #confirmed that at index \"hello\" we're seeing the glove vector for \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20002, 200)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_data(s, length):\n",
    "    padded = np.zeros((length,), dtype = np.int64)\n",
    "    if len(s) > length: \n",
    "        padded = s[:length]\n",
    "    else:\n",
    "        padded[:len(s)] = s\n",
    "    return np.array(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'! ! ! ! ! ! ! ! ! _g_anderson : _based she look like a tranny'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer['clean_tweet'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_index(x):\n",
    "    try:\n",
    "        return word2index[x]\n",
    "    except KeyError:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transfer['seq_len'] = [len(x.split(' ')) for x in transfer['clean_tweet']]\n",
    "\n",
    "transfer['numeric'] = [[get_index(y) for y in x.split(' ')] for x in transfer['clean_tweet']]\n",
    "\n",
    "transfer['padded_tweet'] = [pad_data(x, 25) for x in transfer.numeric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13a996748>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a82cd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transfer.seq_len.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subclass the custom dataset class with torch.utils.data.Dataset\n",
    "# implement __len__ and __getitem__ function\n",
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df, label, maxlen=20):\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.df.padded_tweet[idx]\n",
    "        y = self.df[self.label][idx]\n",
    "        lens = self.df.seq_len[idx]\n",
    "        return X,y,lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = VectorizeData(transfer, label = 'hatespeech')\n",
    "\n",
    "dl = DataLoader(data, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23353\n",
       "1     1430\n",
       "Name: hatespeech, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer['hatespeech'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06123410268487989"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1430/23353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, weights, vocab_size, embedding_dim, hidden_dim, output_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.weights = weights\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(self.weights)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.dropout_layer = nn.Dropout(p=0.2)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return(autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)), \\\n",
    "               autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.embedding(batch) \n",
    "        #packed_input = pack_padded_sequence(embeds, lengths)\n",
    "        outputs, (ht, ct) = self.lstm(embeds, self.hidden)\n",
    "        # ht is the last hidden state of the sequences\n",
    "        # ht = (1 x batch_size x hidden_dim)\n",
    "        # ht[-1] = (batch_size x hidden_dim)\n",
    "        output = self.dropout_layer(ht[-1])\n",
    "        output = self.hidden2out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 200 \n",
    "num_classes = 2\n",
    "learning_rate = 0.005\n",
    "num_epochs = 2\n",
    "batch_size = 32\n",
    "weights = torch.FloatTensor(weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    23353\n",
       "1     1430\n",
       "Name: hatespeech, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfer.hatespeech.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = LSTMClassifier(weights, total_words, hidden_size, hidden_size, num_classes, batch_size)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.NLLLoss(weight = torch.Tensor([.06,1]))  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4/774], Loss: 0.8619\n",
      "Epoch [1/5], Step [8/774], Loss: 0.7357\n",
      "Epoch [1/5], Step [12/774], Loss: 0.7169\n",
      "Epoch [1/5], Step [16/774], Loss: 0.7117\n",
      "Epoch [1/5], Step [20/774], Loss: 0.9054\n",
      "Epoch [1/5], Step [24/774], Loss: 0.7216\n",
      "Epoch [1/5], Step [28/774], Loss: 0.7002\n",
      "Epoch [1/5], Step [32/774], Loss: 0.6475\n",
      "Epoch [1/5], Step [36/774], Loss: 0.6830\n",
      "Epoch [1/5], Step [40/774], Loss: 0.5765\n",
      "Epoch [1/5], Step [44/774], Loss: 1.0086\n",
      "Epoch [1/5], Step [48/774], Loss: 0.5670\n",
      "Epoch [1/5], Step [52/774], Loss: 0.7761\n",
      "Epoch [1/5], Step [56/774], Loss: 0.8844\n",
      "Epoch [1/5], Step [60/774], Loss: 0.7536\n",
      "Epoch [1/5], Step [64/774], Loss: 1.1917\n",
      "Epoch [1/5], Step [68/774], Loss: 0.6692\n",
      "Epoch [1/5], Step [72/774], Loss: 0.7396\n",
      "Epoch [1/5], Step [76/774], Loss: 0.6005\n",
      "Epoch [1/5], Step [80/774], Loss: 0.6314\n",
      "Epoch [1/5], Step [84/774], Loss: 0.8198\n",
      "Epoch [1/5], Step [88/774], Loss: 0.6671\n",
      "Epoch [1/5], Step [92/774], Loss: 0.4528\n",
      "Epoch [1/5], Step [96/774], Loss: 0.9378\n",
      "Epoch [1/5], Step [100/774], Loss: 0.7631\n",
      "Epoch [1/5], Step [104/774], Loss: 0.6549\n",
      "Epoch [1/5], Step [108/774], Loss: 0.8322\n",
      "Epoch [1/5], Step [112/774], Loss: 0.6511\n",
      "Epoch [1/5], Step [116/774], Loss: 0.6874\n",
      "Epoch [1/5], Step [120/774], Loss: 0.6747\n",
      "Epoch [1/5], Step [124/774], Loss: 0.7048\n",
      "Epoch [1/5], Step [128/774], Loss: 0.6904\n",
      "Epoch [1/5], Step [132/774], Loss: 0.7382\n",
      "Epoch [1/5], Step [136/774], Loss: 0.6982\n",
      "Epoch [1/5], Step [140/774], Loss: 0.6766\n",
      "Epoch [1/5], Step [144/774], Loss: 0.6674\n",
      "Epoch [1/5], Step [148/774], Loss: 0.7266\n",
      "Epoch [1/5], Step [152/774], Loss: 0.6985\n",
      "Epoch [1/5], Step [156/774], Loss: 0.6940\n",
      "Epoch [1/5], Step [160/774], Loss: 0.7086\n",
      "Epoch [1/5], Step [164/774], Loss: 0.6653\n",
      "Epoch [1/5], Step [168/774], Loss: 0.6760\n",
      "Epoch [1/5], Step [172/774], Loss: 0.8089\n",
      "Epoch [1/5], Step [176/774], Loss: 0.6577\n",
      "Epoch [1/5], Step [180/774], Loss: 0.8561\n",
      "Epoch [1/5], Step [184/774], Loss: 0.9441\n",
      "Epoch [1/5], Step [188/774], Loss: 0.4883\n",
      "Epoch [1/5], Step [192/774], Loss: 0.7779\n",
      "Epoch [1/5], Step [196/774], Loss: 0.6748\n",
      "Epoch [1/5], Step [200/774], Loss: 0.6975\n",
      "Epoch [1/5], Step [204/774], Loss: 0.6663\n",
      "Epoch [1/5], Step [208/774], Loss: 0.6496\n",
      "Epoch [1/5], Step [212/774], Loss: 0.6465\n",
      "Epoch [1/5], Step [216/774], Loss: 0.6413\n",
      "Epoch [1/5], Step [220/774], Loss: 0.6154\n",
      "Epoch [1/5], Step [224/774], Loss: 0.7509\n",
      "Epoch [1/5], Step [228/774], Loss: 1.0178\n",
      "Epoch [1/5], Step [232/774], Loss: 0.6949\n",
      "Epoch [1/5], Step [236/774], Loss: 0.7661\n",
      "Epoch [1/5], Step [240/774], Loss: 0.6984\n",
      "Epoch [1/5], Step [244/774], Loss: 0.7000\n",
      "Epoch [1/5], Step [248/774], Loss: 0.6951\n",
      "Epoch [1/5], Step [252/774], Loss: 0.7584\n",
      "Epoch [1/5], Step [256/774], Loss: 0.5252\n",
      "Epoch [1/5], Step [260/774], Loss: 0.6360\n",
      "Epoch [1/5], Step [264/774], Loss: 0.4103\n",
      "Epoch [1/5], Step [268/774], Loss: 0.7888\n",
      "Epoch [1/5], Step [272/774], Loss: 0.7662\n",
      "Epoch [1/5], Step [276/774], Loss: 0.7489\n",
      "Epoch [1/5], Step [280/774], Loss: 0.6708\n",
      "Epoch [1/5], Step [284/774], Loss: 0.6715\n",
      "Epoch [1/5], Step [288/774], Loss: 0.6692\n",
      "Epoch [1/5], Step [292/774], Loss: 0.6607\n",
      "Epoch [1/5], Step [296/774], Loss: 0.6169\n",
      "Epoch [1/5], Step [300/774], Loss: 0.7220\n",
      "Epoch [1/5], Step [304/774], Loss: 0.7164\n",
      "Epoch [1/5], Step [308/774], Loss: 0.6816\n",
      "Epoch [1/5], Step [312/774], Loss: 0.6401\n",
      "Epoch [1/5], Step [316/774], Loss: 1.0106\n",
      "Epoch [1/5], Step [320/774], Loss: 0.6638\n",
      "Epoch [1/5], Step [324/774], Loss: 0.5426\n",
      "Epoch [1/5], Step [328/774], Loss: 0.7388\n",
      "Epoch [1/5], Step [332/774], Loss: 0.6962\n",
      "Epoch [1/5], Step [336/774], Loss: 0.7102\n",
      "Epoch [1/5], Step [340/774], Loss: 0.6555\n",
      "Epoch [1/5], Step [344/774], Loss: 0.7076\n",
      "Epoch [1/5], Step [348/774], Loss: 0.7073\n",
      "Epoch [1/5], Step [352/774], Loss: 0.6980\n",
      "Epoch [1/5], Step [356/774], Loss: 0.6434\n",
      "Epoch [1/5], Step [360/774], Loss: 0.6317\n",
      "Epoch [1/5], Step [364/774], Loss: 0.7450\n",
      "Epoch [1/5], Step [368/774], Loss: 0.6759\n",
      "Epoch [1/5], Step [372/774], Loss: 0.5397\n",
      "Epoch [1/5], Step [376/774], Loss: 0.6306\n",
      "Epoch [1/5], Step [380/774], Loss: 0.6914\n",
      "Epoch [1/5], Step [384/774], Loss: 0.6699\n",
      "Epoch [1/5], Step [388/774], Loss: 0.6732\n",
      "Epoch [1/5], Step [392/774], Loss: 0.6334\n",
      "Epoch [1/5], Step [396/774], Loss: 0.6718\n",
      "Epoch [1/5], Step [400/774], Loss: 0.6698\n",
      "Epoch [1/5], Step [404/774], Loss: 0.6998\n",
      "Epoch [1/5], Step [408/774], Loss: 0.5349\n",
      "Epoch [1/5], Step [412/774], Loss: 0.7518\n",
      "Epoch [1/5], Step [416/774], Loss: 0.4267\n",
      "Epoch [1/5], Step [420/774], Loss: 0.6393\n",
      "Epoch [1/5], Step [424/774], Loss: 0.3144\n",
      "Epoch [1/5], Step [428/774], Loss: 0.3277\n",
      "Epoch [1/5], Step [432/774], Loss: 0.7465\n",
      "Epoch [1/5], Step [436/774], Loss: 0.8213\n",
      "Epoch [1/5], Step [440/774], Loss: 0.6969\n",
      "Epoch [1/5], Step [444/774], Loss: 0.7773\n",
      "Epoch [1/5], Step [448/774], Loss: 0.7832\n",
      "Epoch [1/5], Step [452/774], Loss: 0.6177\n",
      "Epoch [1/5], Step [456/774], Loss: 0.5581\n",
      "Epoch [1/5], Step [460/774], Loss: 0.6854\n",
      "Epoch [1/5], Step [464/774], Loss: 0.7686\n",
      "Epoch [1/5], Step [468/774], Loss: 0.6957\n",
      "Epoch [1/5], Step [472/774], Loss: 0.7031\n",
      "Epoch [1/5], Step [476/774], Loss: 0.6915\n",
      "Epoch [1/5], Step [480/774], Loss: 0.7147\n",
      "Epoch [1/5], Step [484/774], Loss: 0.6373\n",
      "Epoch [1/5], Step [488/774], Loss: 0.7192\n",
      "Epoch [1/5], Step [492/774], Loss: 0.6412\n",
      "Epoch [1/5], Step [496/774], Loss: 0.7560\n",
      "Epoch [1/5], Step [500/774], Loss: 0.6221\n",
      "Epoch [1/5], Step [504/774], Loss: 0.5076\n",
      "Epoch [1/5], Step [508/774], Loss: 0.5251\n",
      "Epoch [1/5], Step [512/774], Loss: 0.5149\n",
      "Epoch [1/5], Step [516/774], Loss: 0.6987\n",
      "Epoch [1/5], Step [520/774], Loss: 0.6718\n",
      "Epoch [1/5], Step [524/774], Loss: 0.7334\n",
      "Epoch [1/5], Step [528/774], Loss: 0.6518\n",
      "Epoch [1/5], Step [532/774], Loss: 0.6763\n",
      "Epoch [1/5], Step [536/774], Loss: 0.6322\n",
      "Epoch [1/5], Step [540/774], Loss: 0.6521\n",
      "Epoch [1/5], Step [544/774], Loss: 0.4594\n",
      "Epoch [1/5], Step [548/774], Loss: 0.8654\n",
      "Epoch [1/5], Step [552/774], Loss: 0.6686\n",
      "Epoch [1/5], Step [556/774], Loss: 0.6153\n",
      "Epoch [1/5], Step [560/774], Loss: 0.4466\n",
      "Epoch [1/5], Step [564/774], Loss: 0.4169\n",
      "Epoch [1/5], Step [568/774], Loss: 0.8044\n",
      "Epoch [1/5], Step [572/774], Loss: 0.7457\n",
      "Epoch [1/5], Step [576/774], Loss: 0.6695\n",
      "Epoch [1/5], Step [580/774], Loss: 0.6903\n",
      "Epoch [1/5], Step [584/774], Loss: 0.6974\n",
      "Epoch [1/5], Step [588/774], Loss: 0.6857\n",
      "Epoch [1/5], Step [592/774], Loss: 0.7000\n",
      "Epoch [1/5], Step [596/774], Loss: 0.8162\n",
      "Epoch [1/5], Step [600/774], Loss: 0.6485\n",
      "Epoch [1/5], Step [604/774], Loss: 0.6026\n",
      "Epoch [1/5], Step [608/774], Loss: 0.8215\n",
      "Epoch [1/5], Step [612/774], Loss: 0.6379\n",
      "Epoch [1/5], Step [616/774], Loss: 0.6608\n",
      "Epoch [1/5], Step [620/774], Loss: 0.5711\n",
      "Epoch [1/5], Step [624/774], Loss: 0.5813\n",
      "Epoch [1/5], Step [628/774], Loss: 0.6365\n",
      "Epoch [1/5], Step [632/774], Loss: 0.5319\n",
      "Epoch [1/5], Step [636/774], Loss: 0.5111\n",
      "Epoch [1/5], Step [640/774], Loss: 0.4812\n",
      "Epoch [1/5], Step [644/774], Loss: 0.3169\n",
      "Epoch [1/5], Step [648/774], Loss: 0.3121\n",
      "Epoch [1/5], Step [652/774], Loss: 0.6374\n",
      "Epoch [1/5], Step [656/774], Loss: 0.7694\n",
      "Epoch [1/5], Step [660/774], Loss: 0.8970\n",
      "Epoch [1/5], Step [664/774], Loss: 0.8060\n",
      "Epoch [1/5], Step [668/774], Loss: 0.8809\n",
      "Epoch [1/5], Step [672/774], Loss: 0.6667\n",
      "Epoch [1/5], Step [676/774], Loss: 0.7721\n",
      "Epoch [1/5], Step [680/774], Loss: 0.6204\n",
      "Epoch [1/5], Step [684/774], Loss: 0.5384\n",
      "Epoch [1/5], Step [688/774], Loss: 0.8738\n",
      "Epoch [1/5], Step [692/774], Loss: 0.7900\n",
      "Epoch [1/5], Step [696/774], Loss: 0.6504\n",
      "Epoch [1/5], Step [700/774], Loss: 0.5984\n",
      "Epoch [1/5], Step [704/774], Loss: 0.5632\n",
      "Epoch [1/5], Step [708/774], Loss: 0.8972\n",
      "Epoch [1/5], Step [712/774], Loss: 0.4786\n",
      "Epoch [1/5], Step [716/774], Loss: 0.5154\n",
      "Epoch [1/5], Step [720/774], Loss: 0.5276\n",
      "Epoch [1/5], Step [724/774], Loss: 0.6353\n",
      "Epoch [1/5], Step [728/774], Loss: 0.4361\n",
      "Epoch [1/5], Step [732/774], Loss: 0.5767\n",
      "Epoch [1/5], Step [736/774], Loss: 0.6313\n",
      "Epoch [1/5], Step [740/774], Loss: 0.8839\n",
      "Epoch [1/5], Step [744/774], Loss: 0.5446\n",
      "Epoch [1/5], Step [748/774], Loss: 0.6343\n",
      "Epoch [1/5], Step [752/774], Loss: 0.6921\n",
      "Epoch [1/5], Step [756/774], Loss: 0.4040\n",
      "Epoch [1/5], Step [760/774], Loss: 0.5826\n",
      "Epoch [1/5], Step [764/774], Loss: 0.4117\n",
      "Epoch [1/5], Step [768/774], Loss: 0.5914\n",
      "Epoch [1/5], Step [772/774], Loss: 0.8252\n",
      "Epoch [2/5], Step [4/774], Loss: 0.5521\n",
      "Epoch [2/5], Step [8/774], Loss: 0.7042\n",
      "Epoch [2/5], Step [12/774], Loss: 0.8542\n",
      "Epoch [2/5], Step [16/774], Loss: 0.5865\n",
      "Epoch [2/5], Step [20/774], Loss: 0.6309\n",
      "Epoch [2/5], Step [24/774], Loss: 0.4010\n",
      "Epoch [2/5], Step [28/774], Loss: 0.4146\n",
      "Epoch [2/5], Step [32/774], Loss: 0.6605\n",
      "Epoch [2/5], Step [36/774], Loss: 0.1956\n",
      "Epoch [2/5], Step [40/774], Loss: 0.7259\n",
      "Epoch [2/5], Step [44/774], Loss: 0.5902\n",
      "Epoch [2/5], Step [48/774], Loss: 0.7354\n",
      "Epoch [2/5], Step [52/774], Loss: 0.3523\n",
      "Epoch [2/5], Step [56/774], Loss: 0.5930\n",
      "Epoch [2/5], Step [60/774], Loss: 0.9532\n",
      "Epoch [2/5], Step [64/774], Loss: 0.6121\n",
      "Epoch [2/5], Step [68/774], Loss: 0.7001\n",
      "Epoch [2/5], Step [72/774], Loss: 0.5509\n",
      "Epoch [2/5], Step [76/774], Loss: 0.4125\n",
      "Epoch [2/5], Step [80/774], Loss: 0.3852\n",
      "Epoch [2/5], Step [84/774], Loss: 0.2224\n",
      "Epoch [2/5], Step [88/774], Loss: 0.3439\n",
      "Epoch [2/5], Step [92/774], Loss: 0.2562\n",
      "Epoch [2/5], Step [96/774], Loss: 0.7582\n",
      "Epoch [2/5], Step [100/774], Loss: 0.3929\n",
      "Epoch [2/5], Step [104/774], Loss: 0.3354\n",
      "Epoch [2/5], Step [108/774], Loss: 0.2725\n",
      "Epoch [2/5], Step [112/774], Loss: 0.5936\n",
      "Epoch [2/5], Step [116/774], Loss: 0.7863\n",
      "Epoch [2/5], Step [120/774], Loss: 0.5245\n",
      "Epoch [2/5], Step [124/774], Loss: 0.4281\n",
      "Epoch [2/5], Step [128/774], Loss: 0.3613\n",
      "Epoch [2/5], Step [132/774], Loss: 0.8091\n",
      "Epoch [2/5], Step [136/774], Loss: 0.5748\n",
      "Epoch [2/5], Step [140/774], Loss: 0.7295\n",
      "Epoch [2/5], Step [144/774], Loss: 0.4969\n",
      "Epoch [2/5], Step [148/774], Loss: 0.3594\n",
      "Epoch [2/5], Step [152/774], Loss: 0.3121\n",
      "Epoch [2/5], Step [156/774], Loss: 0.2203\n",
      "Epoch [2/5], Step [160/774], Loss: 0.2424\n",
      "Epoch [2/5], Step [164/774], Loss: 0.7145\n",
      "Epoch [2/5], Step [168/774], Loss: 0.2409\n",
      "Epoch [2/5], Step [172/774], Loss: 0.3613\n",
      "Epoch [2/5], Step [176/774], Loss: 0.7503\n",
      "Epoch [2/5], Step [180/774], Loss: 0.7197\n",
      "Epoch [2/5], Step [184/774], Loss: 0.5608\n",
      "Epoch [2/5], Step [188/774], Loss: 0.2330\n",
      "Epoch [2/5], Step [192/774], Loss: 0.8460\n",
      "Epoch [2/5], Step [196/774], Loss: 0.9227\n",
      "Epoch [2/5], Step [200/774], Loss: 0.2027\n",
      "Epoch [2/5], Step [204/774], Loss: 0.6111\n",
      "Epoch [2/5], Step [208/774], Loss: 0.5581\n",
      "Epoch [2/5], Step [212/774], Loss: 0.4939\n",
      "Epoch [2/5], Step [216/774], Loss: 0.5940\n",
      "Epoch [2/5], Step [220/774], Loss: 0.3341\n",
      "Epoch [2/5], Step [224/774], Loss: 0.5332\n",
      "Epoch [2/5], Step [228/774], Loss: 0.5348\n",
      "Epoch [2/5], Step [232/774], Loss: 0.4907\n",
      "Epoch [2/5], Step [236/774], Loss: 0.4002\n",
      "Epoch [2/5], Step [240/774], Loss: 0.5583\n",
      "Epoch [2/5], Step [244/774], Loss: 0.2896\n",
      "Epoch [2/5], Step [248/774], Loss: 0.4296\n",
      "Epoch [2/5], Step [252/774], Loss: 0.6529\n",
      "Epoch [2/5], Step [256/774], Loss: 0.3014\n",
      "Epoch [2/5], Step [260/774], Loss: 0.2462\n",
      "Epoch [2/5], Step [264/774], Loss: 0.7724\n",
      "Epoch [2/5], Step [268/774], Loss: 0.5742\n",
      "Epoch [2/5], Step [272/774], Loss: 0.5420\n",
      "Epoch [2/5], Step [276/774], Loss: 0.6412\n",
      "Epoch [2/5], Step [280/774], Loss: 0.4246\n",
      "Epoch [2/5], Step [284/774], Loss: 0.3905\n",
      "Epoch [2/5], Step [288/774], Loss: 0.7042\n",
      "Epoch [2/5], Step [292/774], Loss: 0.6114\n",
      "Epoch [2/5], Step [296/774], Loss: 0.2878\n",
      "Epoch [2/5], Step [300/774], Loss: 0.5765\n",
      "Epoch [2/5], Step [304/774], Loss: 1.0239\n",
      "Epoch [2/5], Step [308/774], Loss: 0.6413\n",
      "Epoch [2/5], Step [312/774], Loss: 0.4778\n",
      "Epoch [2/5], Step [316/774], Loss: 0.3669\n",
      "Epoch [2/5], Step [320/774], Loss: 0.8634\n",
      "Epoch [2/5], Step [324/774], Loss: 0.6710\n",
      "Epoch [2/5], Step [328/774], Loss: 0.3119\n",
      "Epoch [2/5], Step [332/774], Loss: 1.0391\n",
      "Epoch [2/5], Step [336/774], Loss: 0.4154\n",
      "Epoch [2/5], Step [340/774], Loss: 0.5962\n",
      "Epoch [2/5], Step [344/774], Loss: 0.7022\n",
      "Epoch [2/5], Step [348/774], Loss: 0.2534\n",
      "Epoch [2/5], Step [352/774], Loss: 0.6059\n",
      "Epoch [2/5], Step [356/774], Loss: 0.1663\n",
      "Epoch [2/5], Step [360/774], Loss: 0.2150\n",
      "Epoch [2/5], Step [364/774], Loss: 1.0358\n",
      "Epoch [2/5], Step [368/774], Loss: 0.4385\n",
      "Epoch [2/5], Step [372/774], Loss: 0.4168\n",
      "Epoch [2/5], Step [376/774], Loss: 0.5340\n",
      "Epoch [2/5], Step [380/774], Loss: 0.1606\n",
      "Epoch [2/5], Step [384/774], Loss: 0.6777\n",
      "Epoch [2/5], Step [388/774], Loss: 0.5039\n",
      "Epoch [2/5], Step [392/774], Loss: 0.2230\n",
      "Epoch [2/5], Step [396/774], Loss: 0.2376\n",
      "Epoch [2/5], Step [400/774], Loss: 0.1792\n",
      "Epoch [2/5], Step [404/774], Loss: 0.4728\n",
      "Epoch [2/5], Step [408/774], Loss: 0.2252\n",
      "Epoch [2/5], Step [412/774], Loss: 0.5026\n",
      "Epoch [2/5], Step [416/774], Loss: 0.6539\n",
      "Epoch [2/5], Step [420/774], Loss: 0.6965\n",
      "Epoch [2/5], Step [424/774], Loss: 0.7303\n",
      "Epoch [2/5], Step [428/774], Loss: 0.5929\n",
      "Epoch [2/5], Step [432/774], Loss: 0.2737\n",
      "Epoch [2/5], Step [436/774], Loss: 0.2611\n",
      "Epoch [2/5], Step [440/774], Loss: 0.2864\n",
      "Epoch [2/5], Step [444/774], Loss: 0.2620\n",
      "Epoch [2/5], Step [448/774], Loss: 0.1951\n",
      "Epoch [2/5], Step [452/774], Loss: 1.0777\n",
      "Epoch [2/5], Step [456/774], Loss: 1.3262\n",
      "Epoch [2/5], Step [460/774], Loss: 0.2196\n",
      "Epoch [2/5], Step [464/774], Loss: 0.5736\n",
      "Epoch [2/5], Step [468/774], Loss: 0.2552\n",
      "Epoch [2/5], Step [472/774], Loss: 0.2079\n",
      "Epoch [2/5], Step [476/774], Loss: 0.2478\n",
      "Epoch [2/5], Step [480/774], Loss: 0.4967\n",
      "Epoch [2/5], Step [484/774], Loss: 0.5811\n",
      "Epoch [2/5], Step [488/774], Loss: 0.6579\n",
      "Epoch [2/5], Step [492/774], Loss: 0.3621\n",
      "Epoch [2/5], Step [496/774], Loss: 0.3443\n",
      "Epoch [2/5], Step [500/774], Loss: 0.8574\n",
      "Epoch [2/5], Step [504/774], Loss: 0.3480\n",
      "Epoch [2/5], Step [508/774], Loss: 0.6797\n",
      "Epoch [2/5], Step [512/774], Loss: 0.4943\n",
      "Epoch [2/5], Step [516/774], Loss: 0.2746\n",
      "Epoch [2/5], Step [520/774], Loss: 0.1747\n",
      "Epoch [2/5], Step [524/774], Loss: 0.4724\n",
      "Epoch [2/5], Step [528/774], Loss: 0.7631\n",
      "Epoch [2/5], Step [532/774], Loss: 0.1880\n",
      "Epoch [2/5], Step [536/774], Loss: 0.2214\n",
      "Epoch [2/5], Step [540/774], Loss: 1.2845\n",
      "Epoch [2/5], Step [544/774], Loss: 1.0383\n",
      "Epoch [2/5], Step [548/774], Loss: 1.2438\n",
      "Epoch [2/5], Step [552/774], Loss: 0.1731\n",
      "Epoch [2/5], Step [556/774], Loss: 0.5111\n",
      "Epoch [2/5], Step [560/774], Loss: 0.1374\n",
      "Epoch [2/5], Step [564/774], Loss: 0.1748\n",
      "Epoch [2/5], Step [568/774], Loss: 0.4341\n",
      "Epoch [2/5], Step [572/774], Loss: 1.0677\n",
      "Epoch [2/5], Step [576/774], Loss: 0.4397\n",
      "Epoch [2/5], Step [580/774], Loss: 0.4975\n",
      "Epoch [2/5], Step [584/774], Loss: 0.3009\n",
      "Epoch [2/5], Step [588/774], Loss: 0.4818\n",
      "Epoch [2/5], Step [592/774], Loss: 0.7970\n",
      "Epoch [2/5], Step [596/774], Loss: 0.5221\n",
      "Epoch [2/5], Step [600/774], Loss: 0.4340\n",
      "Epoch [2/5], Step [604/774], Loss: 0.5559\n",
      "Epoch [2/5], Step [608/774], Loss: 0.5906\n",
      "Epoch [2/5], Step [612/774], Loss: 0.9180\n",
      "Epoch [2/5], Step [616/774], Loss: 0.3340\n",
      "Epoch [2/5], Step [620/774], Loss: 0.6831\n",
      "Epoch [2/5], Step [624/774], Loss: 0.5042\n",
      "Epoch [2/5], Step [628/774], Loss: 0.3215\n",
      "Epoch [2/5], Step [632/774], Loss: 0.3854\n",
      "Epoch [2/5], Step [636/774], Loss: 0.2928\n",
      "Epoch [2/5], Step [640/774], Loss: 0.7606\n",
      "Epoch [2/5], Step [644/774], Loss: 0.1957\n",
      "Epoch [2/5], Step [648/774], Loss: 0.3580\n",
      "Epoch [2/5], Step [652/774], Loss: 0.2751\n",
      "Epoch [2/5], Step [656/774], Loss: 0.8643\n",
      "Epoch [2/5], Step [660/774], Loss: 0.2998\n",
      "Epoch [2/5], Step [664/774], Loss: 0.2888\n",
      "Epoch [2/5], Step [668/774], Loss: 0.3457\n",
      "Epoch [2/5], Step [672/774], Loss: 0.2445\n",
      "Epoch [2/5], Step [676/774], Loss: 0.5053\n",
      "Epoch [2/5], Step [680/774], Loss: 0.6517\n",
      "Epoch [2/5], Step [684/774], Loss: 0.3299\n",
      "Epoch [2/5], Step [688/774], Loss: 0.6402\n",
      "Epoch [2/5], Step [692/774], Loss: 0.6814\n",
      "Epoch [2/5], Step [696/774], Loss: 0.4202\n",
      "Epoch [2/5], Step [700/774], Loss: 0.8142\n",
      "Epoch [2/5], Step [704/774], Loss: 0.3543\n",
      "Epoch [2/5], Step [708/774], Loss: 0.4157\n",
      "Epoch [2/5], Step [712/774], Loss: 0.8713\n",
      "Epoch [2/5], Step [716/774], Loss: 0.2049\n",
      "Epoch [2/5], Step [720/774], Loss: 0.2302\n",
      "Epoch [2/5], Step [724/774], Loss: 0.4643\n",
      "Epoch [2/5], Step [728/774], Loss: 0.5765\n",
      "Epoch [2/5], Step [732/774], Loss: 0.2537\n",
      "Epoch [2/5], Step [736/774], Loss: 0.2109\n",
      "Epoch [2/5], Step [740/774], Loss: 0.5721\n",
      "Epoch [2/5], Step [744/774], Loss: 0.4073\n",
      "Epoch [2/5], Step [748/774], Loss: 0.3542\n",
      "Epoch [2/5], Step [752/774], Loss: 0.3273\n",
      "Epoch [2/5], Step [756/774], Loss: 0.2657\n",
      "Epoch [2/5], Step [760/774], Loss: 0.3201\n",
      "Epoch [2/5], Step [764/774], Loss: 0.2009\n",
      "Epoch [2/5], Step [768/774], Loss: 0.5899\n",
      "Epoch [2/5], Step [772/774], Loss: 0.2356\n",
      "Epoch [3/5], Step [4/774], Loss: 0.6266\n",
      "Epoch [3/5], Step [8/774], Loss: 0.6823\n",
      "Epoch [3/5], Step [12/774], Loss: 0.6362\n",
      "Epoch [3/5], Step [16/774], Loss: 0.5115\n",
      "Epoch [3/5], Step [20/774], Loss: 0.1976\n",
      "Epoch [3/5], Step [24/774], Loss: 0.4025\n",
      "Epoch [3/5], Step [28/774], Loss: 0.5807\n",
      "Epoch [3/5], Step [32/774], Loss: 0.3629\n",
      "Epoch [3/5], Step [36/774], Loss: 0.3030\n",
      "Epoch [3/5], Step [40/774], Loss: 0.5738\n",
      "Epoch [3/5], Step [44/774], Loss: 0.3389\n",
      "Epoch [3/5], Step [48/774], Loss: 0.1719\n",
      "Epoch [3/5], Step [52/774], Loss: 0.2191\n",
      "Epoch [3/5], Step [56/774], Loss: 0.2442\n",
      "Epoch [3/5], Step [60/774], Loss: 0.3759\n",
      "Epoch [3/5], Step [64/774], Loss: 0.1511\n",
      "Epoch [3/5], Step [68/774], Loss: 0.2187\n",
      "Epoch [3/5], Step [72/774], Loss: 0.1770\n",
      "Epoch [3/5], Step [76/774], Loss: 0.3212\n",
      "Epoch [3/5], Step [80/774], Loss: 1.1307\n",
      "Epoch [3/5], Step [84/774], Loss: 0.6606\n",
      "Epoch [3/5], Step [88/774], Loss: 0.2483\n",
      "Epoch [3/5], Step [92/774], Loss: 0.6384\n",
      "Epoch [3/5], Step [96/774], Loss: 0.3675\n",
      "Epoch [3/5], Step [100/774], Loss: 0.3210\n",
      "Epoch [3/5], Step [104/774], Loss: 0.7262\n",
      "Epoch [3/5], Step [108/774], Loss: 0.4040\n",
      "Epoch [3/5], Step [112/774], Loss: 0.1675\n",
      "Epoch [3/5], Step [116/774], Loss: 0.4232\n",
      "Epoch [3/5], Step [120/774], Loss: 0.2197\n",
      "Epoch [3/5], Step [124/774], Loss: 0.2175\n",
      "Epoch [3/5], Step [128/774], Loss: 0.2375\n",
      "Epoch [3/5], Step [132/774], Loss: 0.1683\n",
      "Epoch [3/5], Step [136/774], Loss: 0.8482\n",
      "Epoch [3/5], Step [140/774], Loss: 0.2084\n",
      "Epoch [3/5], Step [144/774], Loss: 0.3254\n",
      "Epoch [3/5], Step [148/774], Loss: 0.3332\n",
      "Epoch [3/5], Step [152/774], Loss: 0.1885\n",
      "Epoch [3/5], Step [156/774], Loss: 0.2612\n",
      "Epoch [3/5], Step [160/774], Loss: 0.5259\n",
      "Epoch [3/5], Step [164/774], Loss: 0.6344\n",
      "Epoch [3/5], Step [168/774], Loss: 0.2879\n",
      "Epoch [3/5], Step [172/774], Loss: 0.4007\n",
      "Epoch [3/5], Step [176/774], Loss: 0.1425\n",
      "Epoch [3/5], Step [180/774], Loss: 0.2532\n",
      "Epoch [3/5], Step [184/774], Loss: 0.1562\n",
      "Epoch [3/5], Step [188/774], Loss: 0.5586\n",
      "Epoch [3/5], Step [192/774], Loss: 0.3848\n",
      "Epoch [3/5], Step [196/774], Loss: 0.5049\n",
      "Epoch [3/5], Step [200/774], Loss: 0.4136\n",
      "Epoch [3/5], Step [204/774], Loss: 0.2807\n",
      "Epoch [3/5], Step [208/774], Loss: 0.5116\n",
      "Epoch [3/5], Step [212/774], Loss: 0.8153\n",
      "Epoch [3/5], Step [216/774], Loss: 0.1514\n",
      "Epoch [3/5], Step [220/774], Loss: 0.1995\n",
      "Epoch [3/5], Step [224/774], Loss: 0.6566\n",
      "Epoch [3/5], Step [228/774], Loss: 0.9656\n",
      "Epoch [3/5], Step [232/774], Loss: 0.8631\n",
      "Epoch [3/5], Step [236/774], Loss: 0.2135\n",
      "Epoch [3/5], Step [240/774], Loss: 0.1690\n",
      "Epoch [3/5], Step [244/774], Loss: 0.2428\n",
      "Epoch [3/5], Step [248/774], Loss: 0.5271\n",
      "Epoch [3/5], Step [252/774], Loss: 0.6647\n",
      "Epoch [3/5], Step [256/774], Loss: 0.3457\n",
      "Epoch [3/5], Step [260/774], Loss: 0.5824\n",
      "Epoch [3/5], Step [264/774], Loss: 0.2795\n",
      "Epoch [3/5], Step [268/774], Loss: 0.4162\n",
      "Epoch [3/5], Step [272/774], Loss: 0.1454\n",
      "Epoch [3/5], Step [276/774], Loss: 0.1684\n",
      "Epoch [3/5], Step [280/774], Loss: 0.5714\n",
      "Epoch [3/5], Step [284/774], Loss: 0.5604\n",
      "Epoch [3/5], Step [288/774], Loss: 0.3664\n",
      "Epoch [3/5], Step [292/774], Loss: 0.5394\n",
      "Epoch [3/5], Step [296/774], Loss: 0.4010\n",
      "Epoch [3/5], Step [300/774], Loss: 0.8243\n",
      "Epoch [3/5], Step [304/774], Loss: 1.1226\n",
      "Epoch [3/5], Step [308/774], Loss: 0.6577\n",
      "Epoch [3/5], Step [312/774], Loss: 0.4115\n",
      "Epoch [3/5], Step [316/774], Loss: 0.1892\n",
      "Epoch [3/5], Step [320/774], Loss: 0.2876\n",
      "Epoch [3/5], Step [324/774], Loss: 0.1614\n",
      "Epoch [3/5], Step [328/774], Loss: 0.1890\n",
      "Epoch [3/5], Step [332/774], Loss: 0.3075\n",
      "Epoch [3/5], Step [336/774], Loss: 0.4097\n",
      "Epoch [3/5], Step [340/774], Loss: 0.8460\n",
      "Epoch [3/5], Step [344/774], Loss: 0.3897\n",
      "Epoch [3/5], Step [348/774], Loss: 0.3283\n",
      "Epoch [3/5], Step [352/774], Loss: 0.1499\n",
      "Epoch [3/5], Step [356/774], Loss: 0.3715\n",
      "Epoch [3/5], Step [360/774], Loss: 0.3305\n",
      "Epoch [3/5], Step [364/774], Loss: 0.2450\n",
      "Epoch [3/5], Step [368/774], Loss: 0.5664\n",
      "Epoch [3/5], Step [372/774], Loss: 0.1666\n",
      "Epoch [3/5], Step [376/774], Loss: 0.1098\n",
      "Epoch [3/5], Step [380/774], Loss: 0.1981\n",
      "Epoch [3/5], Step [384/774], Loss: 0.2882\n",
      "Epoch [3/5], Step [388/774], Loss: 0.7268\n",
      "Epoch [3/5], Step [392/774], Loss: 0.4590\n",
      "Epoch [3/5], Step [396/774], Loss: 0.5956\n",
      "Epoch [3/5], Step [400/774], Loss: 0.9805\n",
      "Epoch [3/5], Step [404/774], Loss: 0.4016\n",
      "Epoch [3/5], Step [408/774], Loss: 0.3210\n",
      "Epoch [3/5], Step [412/774], Loss: 0.5187\n",
      "Epoch [3/5], Step [416/774], Loss: 0.8307\n",
      "Epoch [3/5], Step [420/774], Loss: 0.1747\n",
      "Epoch [3/5], Step [424/774], Loss: 0.4447\n",
      "Epoch [3/5], Step [428/774], Loss: 0.2773\n",
      "Epoch [3/5], Step [432/774], Loss: 0.3666\n",
      "Epoch [3/5], Step [436/774], Loss: 0.1897\n",
      "Epoch [3/5], Step [440/774], Loss: 0.1788\n",
      "Epoch [3/5], Step [444/774], Loss: 0.2320\n",
      "Epoch [3/5], Step [448/774], Loss: 0.2112\n",
      "Epoch [3/5], Step [452/774], Loss: 0.4373\n",
      "Epoch [3/5], Step [456/774], Loss: 0.2400\n",
      "Epoch [3/5], Step [460/774], Loss: 0.3556\n",
      "Epoch [3/5], Step [464/774], Loss: 0.3515\n",
      "Epoch [3/5], Step [468/774], Loss: 0.4297\n",
      "Epoch [3/5], Step [472/774], Loss: 0.2207\n",
      "Epoch [3/5], Step [476/774], Loss: 0.1945\n",
      "Epoch [3/5], Step [480/774], Loss: 0.3110\n",
      "Epoch [3/5], Step [484/774], Loss: 0.2518\n",
      "Epoch [3/5], Step [488/774], Loss: 0.5969\n",
      "Epoch [3/5], Step [492/774], Loss: 0.4645\n",
      "Epoch [3/5], Step [496/774], Loss: 0.3410\n",
      "Epoch [3/5], Step [500/774], Loss: 0.1996\n",
      "Epoch [3/5], Step [504/774], Loss: 0.3551\n",
      "Epoch [3/5], Step [508/774], Loss: 0.4441\n",
      "Epoch [3/5], Step [512/774], Loss: 0.1852\n",
      "Epoch [3/5], Step [516/774], Loss: 0.3504\n",
      "Epoch [3/5], Step [520/774], Loss: 0.1594\n",
      "Epoch [3/5], Step [524/774], Loss: 0.1085\n",
      "Epoch [3/5], Step [528/774], Loss: 0.1570\n",
      "Epoch [3/5], Step [532/774], Loss: 0.2980\n",
      "Epoch [3/5], Step [536/774], Loss: 0.1529\n",
      "Epoch [3/5], Step [540/774], Loss: 0.2017\n",
      "Epoch [3/5], Step [544/774], Loss: 0.1456\n",
      "Epoch [3/5], Step [548/774], Loss: 0.7705\n",
      "Epoch [3/5], Step [552/774], Loss: 0.5005\n",
      "Epoch [3/5], Step [556/774], Loss: 0.8905\n",
      "Epoch [3/5], Step [560/774], Loss: 1.0969\n",
      "Epoch [3/5], Step [564/774], Loss: 0.5498\n",
      "Epoch [3/5], Step [568/774], Loss: 0.2831\n",
      "Epoch [3/5], Step [572/774], Loss: 0.1886\n",
      "Epoch [3/5], Step [576/774], Loss: 0.2039\n",
      "Epoch [3/5], Step [580/774], Loss: 0.4104\n",
      "Epoch [3/5], Step [584/774], Loss: 0.6926\n",
      "Epoch [3/5], Step [588/774], Loss: 0.8988\n",
      "Epoch [3/5], Step [592/774], Loss: 0.1879\n",
      "Epoch [3/5], Step [596/774], Loss: 0.2206\n",
      "Epoch [3/5], Step [600/774], Loss: 0.2603\n",
      "Epoch [3/5], Step [604/774], Loss: 0.2113\n",
      "Epoch [3/5], Step [608/774], Loss: 0.7368\n",
      "Epoch [3/5], Step [612/774], Loss: 0.6611\n",
      "Epoch [3/5], Step [616/774], Loss: 0.5382\n",
      "Epoch [3/5], Step [620/774], Loss: 0.1398\n",
      "Epoch [3/5], Step [624/774], Loss: 0.4103\n",
      "Epoch [3/5], Step [628/774], Loss: 0.2962\n",
      "Epoch [3/5], Step [632/774], Loss: 0.5462\n",
      "Epoch [3/5], Step [636/774], Loss: 0.3495\n",
      "Epoch [3/5], Step [640/774], Loss: 0.4813\n",
      "Epoch [3/5], Step [644/774], Loss: 0.4390\n",
      "Epoch [3/5], Step [648/774], Loss: 0.6423\n",
      "Epoch [3/5], Step [652/774], Loss: 0.2252\n",
      "Epoch [3/5], Step [656/774], Loss: 0.1171\n",
      "Epoch [3/5], Step [660/774], Loss: 0.1888\n",
      "Epoch [3/5], Step [664/774], Loss: 0.3528\n",
      "Epoch [3/5], Step [668/774], Loss: 0.2849\n",
      "Epoch [3/5], Step [672/774], Loss: 0.9676\n",
      "Epoch [3/5], Step [676/774], Loss: 0.1719\n",
      "Epoch [3/5], Step [680/774], Loss: 0.6369\n",
      "Epoch [3/5], Step [684/774], Loss: 0.1884\n",
      "Epoch [3/5], Step [688/774], Loss: 0.5167\n",
      "Epoch [3/5], Step [692/774], Loss: 0.2398\n",
      "Epoch [3/5], Step [696/774], Loss: 0.8099\n",
      "Epoch [3/5], Step [700/774], Loss: 0.0945\n",
      "Epoch [3/5], Step [704/774], Loss: 0.2048\n",
      "Epoch [3/5], Step [708/774], Loss: 0.2052\n",
      "Epoch [3/5], Step [712/774], Loss: 1.2274\n",
      "Epoch [3/5], Step [716/774], Loss: 0.2391\n",
      "Epoch [3/5], Step [720/774], Loss: 0.1862\n",
      "Epoch [3/5], Step [724/774], Loss: 0.8351\n",
      "Epoch [3/5], Step [728/774], Loss: 0.1942\n",
      "Epoch [3/5], Step [732/774], Loss: 0.4547\n",
      "Epoch [3/5], Step [736/774], Loss: 0.6540\n",
      "Epoch [3/5], Step [740/774], Loss: 0.3387\n",
      "Epoch [3/5], Step [744/774], Loss: 0.2312\n",
      "Epoch [3/5], Step [748/774], Loss: 0.2466\n",
      "Epoch [3/5], Step [752/774], Loss: 0.1566\n",
      "Epoch [3/5], Step [756/774], Loss: 0.1463\n",
      "Epoch [3/5], Step [760/774], Loss: 0.2085\n",
      "Epoch [3/5], Step [764/774], Loss: 0.9975\n",
      "Epoch [3/5], Step [768/774], Loss: 0.4526\n",
      "Epoch [3/5], Step [772/774], Loss: 0.3938\n",
      "Epoch [4/5], Step [4/774], Loss: 0.5399\n",
      "Epoch [4/5], Step [8/774], Loss: 0.6453\n",
      "Epoch [4/5], Step [12/774], Loss: 0.1836\n",
      "Epoch [4/5], Step [16/774], Loss: 0.1275\n",
      "Epoch [4/5], Step [20/774], Loss: 0.3612\n",
      "Epoch [4/5], Step [24/774], Loss: 0.1464\n",
      "Epoch [4/5], Step [28/774], Loss: 0.0862\n",
      "Epoch [4/5], Step [32/774], Loss: 0.1206\n",
      "Epoch [4/5], Step [36/774], Loss: 0.3711\n",
      "Epoch [4/5], Step [40/774], Loss: 1.0974\n",
      "Epoch [4/5], Step [44/774], Loss: 0.3914\n",
      "Epoch [4/5], Step [48/774], Loss: 0.1486\n",
      "Epoch [4/5], Step [52/774], Loss: 0.9222\n",
      "Epoch [4/5], Step [56/774], Loss: 0.4269\n",
      "Epoch [4/5], Step [60/774], Loss: 0.1929\n",
      "Epoch [4/5], Step [64/774], Loss: 0.1231\n",
      "Epoch [4/5], Step [68/774], Loss: 0.1660\n",
      "Epoch [4/5], Step [72/774], Loss: 0.3640\n",
      "Epoch [4/5], Step [76/774], Loss: 0.0759\n",
      "Epoch [4/5], Step [80/774], Loss: 0.7333\n",
      "Epoch [4/5], Step [84/774], Loss: 0.1057\n",
      "Epoch [4/5], Step [88/774], Loss: 0.1332\n",
      "Epoch [4/5], Step [92/774], Loss: 0.1799\n",
      "Epoch [4/5], Step [96/774], Loss: 0.1985\n",
      "Epoch [4/5], Step [100/774], Loss: 0.2186\n",
      "Epoch [4/5], Step [104/774], Loss: 0.5639\n",
      "Epoch [4/5], Step [108/774], Loss: 0.1595\n",
      "Epoch [4/5], Step [112/774], Loss: 0.2346\n",
      "Epoch [4/5], Step [116/774], Loss: 0.1622\n",
      "Epoch [4/5], Step [120/774], Loss: 0.1032\n",
      "Epoch [4/5], Step [124/774], Loss: 0.4222\n",
      "Epoch [4/5], Step [128/774], Loss: 0.1133\n",
      "Epoch [4/5], Step [132/774], Loss: 0.1515\n",
      "Epoch [4/5], Step [136/774], Loss: 0.5012\n",
      "Epoch [4/5], Step [140/774], Loss: 0.1325\n",
      "Epoch [4/5], Step [144/774], Loss: 0.4780\n",
      "Epoch [4/5], Step [148/774], Loss: 0.1950\n",
      "Epoch [4/5], Step [152/774], Loss: 0.2825\n",
      "Epoch [4/5], Step [156/774], Loss: 0.2564\n",
      "Epoch [4/5], Step [160/774], Loss: 0.1527\n",
      "Epoch [4/5], Step [164/774], Loss: 0.3806\n",
      "Epoch [4/5], Step [168/774], Loss: 0.1469\n",
      "Epoch [4/5], Step [172/774], Loss: 0.3429\n",
      "Epoch [4/5], Step [176/774], Loss: 0.1483\n",
      "Epoch [4/5], Step [180/774], Loss: 0.1169\n",
      "Epoch [4/5], Step [184/774], Loss: 0.0815\n",
      "Epoch [4/5], Step [188/774], Loss: 0.1900\n",
      "Epoch [4/5], Step [192/774], Loss: 0.1789\n",
      "Epoch [4/5], Step [196/774], Loss: 0.1102\n",
      "Epoch [4/5], Step [200/774], Loss: 0.1920\n",
      "Epoch [4/5], Step [204/774], Loss: 0.2994\n",
      "Epoch [4/5], Step [208/774], Loss: 0.2141\n",
      "Epoch [4/5], Step [212/774], Loss: 0.6349\n",
      "Epoch [4/5], Step [216/774], Loss: 0.1284\n",
      "Epoch [4/5], Step [220/774], Loss: 0.3746\n",
      "Epoch [4/5], Step [224/774], Loss: 0.1538\n",
      "Epoch [4/5], Step [228/774], Loss: 0.1387\n",
      "Epoch [4/5], Step [232/774], Loss: 0.2986\n",
      "Epoch [4/5], Step [236/774], Loss: 0.0938\n",
      "Epoch [4/5], Step [240/774], Loss: 0.0939\n",
      "Epoch [4/5], Step [244/774], Loss: 0.1075\n",
      "Epoch [4/5], Step [248/774], Loss: 0.0989\n",
      "Epoch [4/5], Step [252/774], Loss: 0.1162\n",
      "Epoch [4/5], Step [256/774], Loss: 0.3006\n",
      "Epoch [4/5], Step [260/774], Loss: 0.5296\n",
      "Epoch [4/5], Step [264/774], Loss: 0.5866\n",
      "Epoch [4/5], Step [268/774], Loss: 0.1477\n",
      "Epoch [4/5], Step [272/774], Loss: 0.2279\n",
      "Epoch [4/5], Step [276/774], Loss: 0.4244\n",
      "Epoch [4/5], Step [280/774], Loss: 0.1593\n",
      "Epoch [4/5], Step [284/774], Loss: 0.2487\n",
      "Epoch [4/5], Step [288/774], Loss: 0.1364\n",
      "Epoch [4/5], Step [292/774], Loss: 0.1917\n",
      "Epoch [4/5], Step [296/774], Loss: 0.7779\n",
      "Epoch [4/5], Step [300/774], Loss: 0.1293\n",
      "Epoch [4/5], Step [304/774], Loss: 0.4027\n",
      "Epoch [4/5], Step [308/774], Loss: 0.4148\n",
      "Epoch [4/5], Step [312/774], Loss: 0.2000\n",
      "Epoch [4/5], Step [316/774], Loss: 0.2936\n",
      "Epoch [4/5], Step [320/774], Loss: 0.2418\n",
      "Epoch [4/5], Step [324/774], Loss: 1.3835\n",
      "Epoch [4/5], Step [328/774], Loss: 0.4599\n",
      "Epoch [4/5], Step [332/774], Loss: 0.5931\n",
      "Epoch [4/5], Step [336/774], Loss: 0.3651\n",
      "Epoch [4/5], Step [340/774], Loss: 0.3379\n",
      "Epoch [4/5], Step [344/774], Loss: 0.3130\n",
      "Epoch [4/5], Step [348/774], Loss: 0.8632\n",
      "Epoch [4/5], Step [352/774], Loss: 0.1948\n",
      "Epoch [4/5], Step [356/774], Loss: 0.2133\n",
      "Epoch [4/5], Step [360/774], Loss: 0.1166\n",
      "Epoch [4/5], Step [364/774], Loss: 0.5539\n",
      "Epoch [4/5], Step [368/774], Loss: 0.1141\n",
      "Epoch [4/5], Step [372/774], Loss: 0.1082\n",
      "Epoch [4/5], Step [376/774], Loss: 0.1768\n",
      "Epoch [4/5], Step [380/774], Loss: 0.5632\n",
      "Epoch [4/5], Step [384/774], Loss: 0.5070\n",
      "Epoch [4/5], Step [388/774], Loss: 0.0945\n",
      "Epoch [4/5], Step [392/774], Loss: 0.2660\n",
      "Epoch [4/5], Step [396/774], Loss: 0.3609\n",
      "Epoch [4/5], Step [400/774], Loss: 0.2376\n",
      "Epoch [4/5], Step [404/774], Loss: 0.2021\n",
      "Epoch [4/5], Step [408/774], Loss: 0.3075\n",
      "Epoch [4/5], Step [412/774], Loss: 0.1837\n",
      "Epoch [4/5], Step [416/774], Loss: 0.9418\n",
      "Epoch [4/5], Step [420/774], Loss: 0.2175\n",
      "Epoch [4/5], Step [424/774], Loss: 0.1371\n",
      "Epoch [4/5], Step [428/774], Loss: 0.1635\n",
      "Epoch [4/5], Step [432/774], Loss: 0.2952\n",
      "Epoch [4/5], Step [436/774], Loss: 0.2359\n",
      "Epoch [4/5], Step [440/774], Loss: 0.3995\n",
      "Epoch [4/5], Step [444/774], Loss: 0.5801\n",
      "Epoch [4/5], Step [448/774], Loss: 0.3284\n",
      "Epoch [4/5], Step [452/774], Loss: 0.1904\n",
      "Epoch [4/5], Step [456/774], Loss: 0.4173\n",
      "Epoch [4/5], Step [460/774], Loss: 0.4693\n",
      "Epoch [4/5], Step [464/774], Loss: 0.3657\n",
      "Epoch [4/5], Step [468/774], Loss: 0.1193\n",
      "Epoch [4/5], Step [472/774], Loss: 0.2591\n",
      "Epoch [4/5], Step [476/774], Loss: 0.3057\n",
      "Epoch [4/5], Step [480/774], Loss: 0.1764\n",
      "Epoch [4/5], Step [484/774], Loss: 0.2003\n",
      "Epoch [4/5], Step [488/774], Loss: 0.2356\n",
      "Epoch [4/5], Step [492/774], Loss: 0.5827\n",
      "Epoch [4/5], Step [496/774], Loss: 0.2155\n",
      "Epoch [4/5], Step [500/774], Loss: 0.1627\n",
      "Epoch [4/5], Step [504/774], Loss: 0.3054\n",
      "Epoch [4/5], Step [508/774], Loss: 0.1113\n",
      "Epoch [4/5], Step [512/774], Loss: 0.0609\n",
      "Epoch [4/5], Step [516/774], Loss: 0.0800\n",
      "Epoch [4/5], Step [520/774], Loss: 0.9353\n",
      "Epoch [4/5], Step [524/774], Loss: 1.6998\n",
      "Epoch [4/5], Step [528/774], Loss: 0.1390\n",
      "Epoch [4/5], Step [532/774], Loss: 0.0832\n",
      "Epoch [4/5], Step [536/774], Loss: 0.4624\n",
      "Epoch [4/5], Step [540/774], Loss: 0.1954\n",
      "Epoch [4/5], Step [544/774], Loss: 0.2085\n",
      "Epoch [4/5], Step [548/774], Loss: 0.2104\n",
      "Epoch [4/5], Step [552/774], Loss: 0.1977\n",
      "Epoch [4/5], Step [556/774], Loss: 0.5509\n",
      "Epoch [4/5], Step [560/774], Loss: 0.2059\n",
      "Epoch [4/5], Step [564/774], Loss: 0.2747\n",
      "Epoch [4/5], Step [568/774], Loss: 1.1442\n",
      "Epoch [4/5], Step [572/774], Loss: 0.2373\n",
      "Epoch [4/5], Step [576/774], Loss: 0.2102\n",
      "Epoch [4/5], Step [580/774], Loss: 0.3583\n",
      "Epoch [4/5], Step [584/774], Loss: 0.3269\n",
      "Epoch [4/5], Step [588/774], Loss: 0.4695\n",
      "Epoch [4/5], Step [592/774], Loss: 0.1734\n",
      "Epoch [4/5], Step [596/774], Loss: 0.2528\n",
      "Epoch [4/5], Step [600/774], Loss: 0.2360\n",
      "Epoch [4/5], Step [604/774], Loss: 0.4813\n",
      "Epoch [4/5], Step [608/774], Loss: 0.4455\n",
      "Epoch [4/5], Step [612/774], Loss: 0.2230\n",
      "Epoch [4/5], Step [616/774], Loss: 0.2459\n",
      "Epoch [4/5], Step [620/774], Loss: 0.6185\n",
      "Epoch [4/5], Step [624/774], Loss: 0.5559\n",
      "Epoch [4/5], Step [628/774], Loss: 0.2912\n",
      "Epoch [4/5], Step [632/774], Loss: 0.1400\n",
      "Epoch [4/5], Step [636/774], Loss: 0.1890\n",
      "Epoch [4/5], Step [640/774], Loss: 0.1424\n",
      "Epoch [4/5], Step [644/774], Loss: 0.3522\n",
      "Epoch [4/5], Step [648/774], Loss: 0.1083\n",
      "Epoch [4/5], Step [652/774], Loss: 1.2329\n",
      "Epoch [4/5], Step [656/774], Loss: 0.3644\n",
      "Epoch [4/5], Step [660/774], Loss: 0.1343\n",
      "Epoch [4/5], Step [664/774], Loss: 0.2115\n",
      "Epoch [4/5], Step [668/774], Loss: 0.1053\n",
      "Epoch [4/5], Step [672/774], Loss: 0.9006\n",
      "Epoch [4/5], Step [676/774], Loss: 0.3350\n",
      "Epoch [4/5], Step [680/774], Loss: 0.5718\n",
      "Epoch [4/5], Step [684/774], Loss: 0.2390\n",
      "Epoch [4/5], Step [688/774], Loss: 0.8928\n",
      "Epoch [4/5], Step [692/774], Loss: 0.1285\n",
      "Epoch [4/5], Step [696/774], Loss: 0.3331\n",
      "Epoch [4/5], Step [700/774], Loss: 0.0927\n",
      "Epoch [4/5], Step [704/774], Loss: 0.2383\n",
      "Epoch [4/5], Step [708/774], Loss: 0.1805\n",
      "Epoch [4/5], Step [712/774], Loss: 0.2933\n",
      "Epoch [4/5], Step [716/774], Loss: 0.1631\n",
      "Epoch [4/5], Step [720/774], Loss: 0.8195\n",
      "Epoch [4/5], Step [724/774], Loss: 0.4877\n",
      "Epoch [4/5], Step [728/774], Loss: 0.6704\n",
      "Epoch [4/5], Step [732/774], Loss: 0.1585\n",
      "Epoch [4/5], Step [736/774], Loss: 0.3572\n",
      "Epoch [4/5], Step [740/774], Loss: 0.2202\n",
      "Epoch [4/5], Step [744/774], Loss: 0.2819\n",
      "Epoch [4/5], Step [748/774], Loss: 0.2008\n",
      "Epoch [4/5], Step [752/774], Loss: 0.0817\n",
      "Epoch [4/5], Step [756/774], Loss: 0.6898\n",
      "Epoch [4/5], Step [760/774], Loss: 0.2355\n",
      "Epoch [4/5], Step [764/774], Loss: 0.4146\n",
      "Epoch [4/5], Step [768/774], Loss: 0.1543\n",
      "Epoch [4/5], Step [772/774], Loss: 0.1048\n",
      "Epoch [5/5], Step [4/774], Loss: 0.1503\n",
      "Epoch [5/5], Step [8/774], Loss: 0.5016\n",
      "Epoch [5/5], Step [12/774], Loss: 0.3828\n",
      "Epoch [5/5], Step [16/774], Loss: 0.6520\n",
      "Epoch [5/5], Step [20/774], Loss: 0.1781\n",
      "Epoch [5/5], Step [24/774], Loss: 0.3294\n",
      "Epoch [5/5], Step [28/774], Loss: 0.1951\n",
      "Epoch [5/5], Step [32/774], Loss: 0.2226\n",
      "Epoch [5/5], Step [36/774], Loss: 0.2654\n",
      "Epoch [5/5], Step [40/774], Loss: 0.0972\n",
      "Epoch [5/5], Step [44/774], Loss: 0.4269\n",
      "Epoch [5/5], Step [48/774], Loss: 0.1471\n",
      "Epoch [5/5], Step [52/774], Loss: 0.8288\n",
      "Epoch [5/5], Step [56/774], Loss: 0.2647\n",
      "Epoch [5/5], Step [60/774], Loss: 0.4574\n",
      "Epoch [5/5], Step [64/774], Loss: 0.6029\n",
      "Epoch [5/5], Step [68/774], Loss: 0.1822\n",
      "Epoch [5/5], Step [72/774], Loss: 0.1919\n",
      "Epoch [5/5], Step [76/774], Loss: 0.1725\n",
      "Epoch [5/5], Step [80/774], Loss: 0.1428\n",
      "Epoch [5/5], Step [84/774], Loss: 0.0418\n",
      "Epoch [5/5], Step [88/774], Loss: 0.3059\n",
      "Epoch [5/5], Step [92/774], Loss: 0.1847\n",
      "Epoch [5/5], Step [96/774], Loss: 0.3009\n",
      "Epoch [5/5], Step [100/774], Loss: 0.1873\n",
      "Epoch [5/5], Step [104/774], Loss: 0.5041\n",
      "Epoch [5/5], Step [108/774], Loss: 0.1353\n",
      "Epoch [5/5], Step [112/774], Loss: 0.1318\n",
      "Epoch [5/5], Step [116/774], Loss: 0.0580\n",
      "Epoch [5/5], Step [120/774], Loss: 0.1756\n",
      "Epoch [5/5], Step [124/774], Loss: 0.5076\n",
      "Epoch [5/5], Step [128/774], Loss: 0.1329\n",
      "Epoch [5/5], Step [132/774], Loss: 0.0845\n",
      "Epoch [5/5], Step [136/774], Loss: 1.5586\n",
      "Epoch [5/5], Step [140/774], Loss: 1.0963\n",
      "Epoch [5/5], Step [144/774], Loss: 0.2595\n",
      "Epoch [5/5], Step [148/774], Loss: 0.4005\n",
      "Epoch [5/5], Step [152/774], Loss: 0.1365\n",
      "Epoch [5/5], Step [156/774], Loss: 0.2380\n",
      "Epoch [5/5], Step [160/774], Loss: 0.3137\n",
      "Epoch [5/5], Step [164/774], Loss: 0.2128\n",
      "Epoch [5/5], Step [168/774], Loss: 0.3247\n",
      "Epoch [5/5], Step [172/774], Loss: 0.1780\n",
      "Epoch [5/5], Step [176/774], Loss: 0.2448\n",
      "Epoch [5/5], Step [180/774], Loss: 0.2374\n",
      "Epoch [5/5], Step [184/774], Loss: 0.8250\n",
      "Epoch [5/5], Step [188/774], Loss: 0.1587\n",
      "Epoch [5/5], Step [192/774], Loss: 0.2927\n",
      "Epoch [5/5], Step [196/774], Loss: 0.1786\n",
      "Epoch [5/5], Step [200/774], Loss: 0.0961\n",
      "Epoch [5/5], Step [204/774], Loss: 0.5597\n",
      "Epoch [5/5], Step [208/774], Loss: 0.0753\n",
      "Epoch [5/5], Step [212/774], Loss: 0.1721\n",
      "Epoch [5/5], Step [216/774], Loss: 0.2010\n",
      "Epoch [5/5], Step [220/774], Loss: 0.1415\n",
      "Epoch [5/5], Step [224/774], Loss: 0.1100\n",
      "Epoch [5/5], Step [228/774], Loss: 0.1063\n",
      "Epoch [5/5], Step [232/774], Loss: 0.3707\n",
      "Epoch [5/5], Step [236/774], Loss: 0.1477\n",
      "Epoch [5/5], Step [240/774], Loss: 0.1935\n",
      "Epoch [5/5], Step [244/774], Loss: 0.2216\n",
      "Epoch [5/5], Step [248/774], Loss: 0.5876\n",
      "Epoch [5/5], Step [252/774], Loss: 0.1001\n",
      "Epoch [5/5], Step [256/774], Loss: 0.7653\n",
      "Epoch [5/5], Step [260/774], Loss: 0.2577\n",
      "Epoch [5/5], Step [264/774], Loss: 0.2413\n",
      "Epoch [5/5], Step [268/774], Loss: 0.3682\n",
      "Epoch [5/5], Step [272/774], Loss: 0.1390\n",
      "Epoch [5/5], Step [276/774], Loss: 0.0904\n",
      "Epoch [5/5], Step [280/774], Loss: 0.2973\n",
      "Epoch [5/5], Step [284/774], Loss: 0.2427\n",
      "Epoch [5/5], Step [288/774], Loss: 1.0158\n",
      "Epoch [5/5], Step [292/774], Loss: 0.2896\n",
      "Epoch [5/5], Step [296/774], Loss: 0.1388\n",
      "Epoch [5/5], Step [300/774], Loss: 0.2063\n",
      "Epoch [5/5], Step [304/774], Loss: 0.2354\n",
      "Epoch [5/5], Step [308/774], Loss: 0.3000\n",
      "Epoch [5/5], Step [312/774], Loss: 0.1241\n",
      "Epoch [5/5], Step [316/774], Loss: 0.0999\n",
      "Epoch [5/5], Step [320/774], Loss: 0.0840\n",
      "Epoch [5/5], Step [324/774], Loss: 0.0704\n",
      "Epoch [5/5], Step [328/774], Loss: 0.1613\n",
      "Epoch [5/5], Step [332/774], Loss: 0.1166\n",
      "Epoch [5/5], Step [336/774], Loss: 0.2970\n",
      "Epoch [5/5], Step [340/774], Loss: 0.4612\n",
      "Epoch [5/5], Step [344/774], Loss: 0.3178\n",
      "Epoch [5/5], Step [348/774], Loss: 0.1037\n",
      "Epoch [5/5], Step [352/774], Loss: 0.2511\n",
      "Epoch [5/5], Step [356/774], Loss: 0.1690\n",
      "Epoch [5/5], Step [360/774], Loss: 0.1129\n",
      "Epoch [5/5], Step [364/774], Loss: 0.1068\n",
      "Epoch [5/5], Step [368/774], Loss: 0.2083\n",
      "Epoch [5/5], Step [372/774], Loss: 0.1154\n",
      "Epoch [5/5], Step [376/774], Loss: 0.1979\n",
      "Epoch [5/5], Step [380/774], Loss: 0.2607\n",
      "Epoch [5/5], Step [384/774], Loss: 0.0830\n",
      "Epoch [5/5], Step [388/774], Loss: 0.3490\n",
      "Epoch [5/5], Step [392/774], Loss: 0.1400\n",
      "Epoch [5/5], Step [396/774], Loss: 0.1119\n",
      "Epoch [5/5], Step [400/774], Loss: 0.2012\n",
      "Epoch [5/5], Step [404/774], Loss: 0.0870\n",
      "Epoch [5/5], Step [408/774], Loss: 0.0916\n",
      "Epoch [5/5], Step [412/774], Loss: 0.1617\n",
      "Epoch [5/5], Step [416/774], Loss: 0.1876\n",
      "Epoch [5/5], Step [420/774], Loss: 0.5129\n",
      "Epoch [5/5], Step [424/774], Loss: 0.2243\n",
      "Epoch [5/5], Step [428/774], Loss: 0.3175\n",
      "Epoch [5/5], Step [432/774], Loss: 0.3382\n",
      "Epoch [5/5], Step [436/774], Loss: 0.0962\n",
      "Epoch [5/5], Step [440/774], Loss: 0.1453\n",
      "Epoch [5/5], Step [444/774], Loss: 0.4631\n",
      "Epoch [5/5], Step [448/774], Loss: 0.4236\n",
      "Epoch [5/5], Step [452/774], Loss: 0.1942\n",
      "Epoch [5/5], Step [456/774], Loss: 0.1008\n",
      "Epoch [5/5], Step [460/774], Loss: 0.2147\n",
      "Epoch [5/5], Step [464/774], Loss: 0.2543\n",
      "Epoch [5/5], Step [468/774], Loss: 0.0317\n",
      "Epoch [5/5], Step [472/774], Loss: 0.6369\n",
      "Epoch [5/5], Step [476/774], Loss: 0.1061\n",
      "Epoch [5/5], Step [480/774], Loss: 0.7631\n",
      "Epoch [5/5], Step [484/774], Loss: 0.1111\n",
      "Epoch [5/5], Step [488/774], Loss: 0.1524\n",
      "Epoch [5/5], Step [492/774], Loss: 0.1024\n",
      "Epoch [5/5], Step [496/774], Loss: 0.1426\n",
      "Epoch [5/5], Step [500/774], Loss: 0.1771\n",
      "Epoch [5/5], Step [504/774], Loss: 0.3988\n",
      "Epoch [5/5], Step [508/774], Loss: 0.2719\n",
      "Epoch [5/5], Step [512/774], Loss: 0.1550\n",
      "Epoch [5/5], Step [516/774], Loss: 0.5339\n",
      "Epoch [5/5], Step [520/774], Loss: 0.1102\n",
      "Epoch [5/5], Step [524/774], Loss: 0.2177\n",
      "Epoch [5/5], Step [528/774], Loss: 0.2399\n",
      "Epoch [5/5], Step [532/774], Loss: 0.3432\n",
      "Epoch [5/5], Step [536/774], Loss: 0.0833\n",
      "Epoch [5/5], Step [540/774], Loss: 0.1705\n",
      "Epoch [5/5], Step [544/774], Loss: 0.2226\n",
      "Epoch [5/5], Step [548/774], Loss: 0.1904\n",
      "Epoch [5/5], Step [552/774], Loss: 0.1851\n",
      "Epoch [5/5], Step [556/774], Loss: 0.0706\n",
      "Epoch [5/5], Step [560/774], Loss: 0.2013\n",
      "Epoch [5/5], Step [564/774], Loss: 0.1678\n",
      "Epoch [5/5], Step [568/774], Loss: 0.7199\n",
      "Epoch [5/5], Step [572/774], Loss: 0.0746\n",
      "Epoch [5/5], Step [576/774], Loss: 0.1959\n",
      "Epoch [5/5], Step [580/774], Loss: 0.0743\n",
      "Epoch [5/5], Step [584/774], Loss: 0.1559\n",
      "Epoch [5/5], Step [588/774], Loss: 1.2778\n",
      "Epoch [5/5], Step [592/774], Loss: 0.1865\n",
      "Epoch [5/5], Step [596/774], Loss: 0.1054\n",
      "Epoch [5/5], Step [600/774], Loss: 0.2404\n",
      "Epoch [5/5], Step [604/774], Loss: 0.1319\n",
      "Epoch [5/5], Step [608/774], Loss: 0.1504\n",
      "Epoch [5/5], Step [612/774], Loss: 0.1226\n",
      "Epoch [5/5], Step [616/774], Loss: 0.2211\n",
      "Epoch [5/5], Step [620/774], Loss: 0.0796\n",
      "Epoch [5/5], Step [624/774], Loss: 0.2347\n",
      "Epoch [5/5], Step [628/774], Loss: 0.3279\n",
      "Epoch [5/5], Step [632/774], Loss: 0.2226\n",
      "Epoch [5/5], Step [636/774], Loss: 0.2698\n",
      "Epoch [5/5], Step [640/774], Loss: 0.1862\n",
      "Epoch [5/5], Step [644/774], Loss: 0.4017\n",
      "Epoch [5/5], Step [648/774], Loss: 0.1609\n",
      "Epoch [5/5], Step [652/774], Loss: 0.9508\n",
      "Epoch [5/5], Step [656/774], Loss: 0.0646\n",
      "Epoch [5/5], Step [660/774], Loss: 0.3830\n",
      "Epoch [5/5], Step [664/774], Loss: 0.0854\n",
      "Epoch [5/5], Step [668/774], Loss: 0.1594\n",
      "Epoch [5/5], Step [672/774], Loss: 0.0856\n",
      "Epoch [5/5], Step [676/774], Loss: 0.1175\n",
      "Epoch [5/5], Step [680/774], Loss: 0.1181\n",
      "Epoch [5/5], Step [684/774], Loss: 0.2798\n",
      "Epoch [5/5], Step [688/774], Loss: 0.0963\n",
      "Epoch [5/5], Step [692/774], Loss: 0.2782\n",
      "Epoch [5/5], Step [696/774], Loss: 0.2176\n",
      "Epoch [5/5], Step [700/774], Loss: 0.1248\n",
      "Epoch [5/5], Step [704/774], Loss: 0.2018\n",
      "Epoch [5/5], Step [708/774], Loss: 0.0255\n",
      "Epoch [5/5], Step [712/774], Loss: 0.0671\n",
      "Epoch [5/5], Step [716/774], Loss: 0.1339\n",
      "Epoch [5/5], Step [720/774], Loss: 0.2346\n",
      "Epoch [5/5], Step [724/774], Loss: 0.0421\n",
      "Epoch [5/5], Step [728/774], Loss: 0.3171\n",
      "Epoch [5/5], Step [732/774], Loss: 0.2615\n",
      "Epoch [5/5], Step [736/774], Loss: 0.2728\n",
      "Epoch [5/5], Step [740/774], Loss: 0.1398\n",
      "Epoch [5/5], Step [744/774], Loss: 0.4233\n",
      "Epoch [5/5], Step [748/774], Loss: 0.1427\n",
      "Epoch [5/5], Step [752/774], Loss: 0.1087\n",
      "Epoch [5/5], Step [756/774], Loss: 0.2064\n",
      "Epoch [5/5], Step [760/774], Loss: 0.2109\n",
      "Epoch [5/5], Step [764/774], Loss: 0.2255\n",
      "Epoch [5/5], Step [768/774], Loss: 0.0890\n",
      "Epoch [5/5], Step [772/774], Loss: 0.1149\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "num_batch = len(dl) - 1\n",
    "# transfer the Model\n",
    "for epoch in range(num_epochs):\n",
    "    it = iter(dl)\n",
    "    # Loop over all batches\n",
    "    for i in range(num_batch):\n",
    "        batch_x,batch_y,batch_len = next(it)\n",
    "        tweets = Variable(batch_x.transpose(0,1))\n",
    "        labels = Variable(batch_y)\n",
    "        lengths = Variable(batch_len)\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(tweets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(transfer.clean_tweet)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11ac0b908>]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ab28c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses) #trained at lr of 0.01 for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59859950304946907"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "pred_labels = []\n",
    "#get training predictions and training f-score\n",
    "it = iter(dl)\n",
    "num_batch = len(dl) - 1\n",
    "# Loop over all batches\n",
    "for i in range(num_batch):\n",
    "    batch_x,batch_y,batch_len = next(it)\n",
    "    tweets = Variable(batch_x.transpose(0,1))\n",
    "    labels = Variable(batch_y)\n",
    "    lengths = Variable(batch_len)\n",
    "    outputs = net(tweets)\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    predictions.extend(list(pred.numpy()))\n",
    "    pred_labels.extend(list(labels.data.numpy()))\n",
    "f1_score(predictions, pred_labels) #almost .6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'model_for_transfer_learning.sav'\n",
    "pickle.dump(net, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
