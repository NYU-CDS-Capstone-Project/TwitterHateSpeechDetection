{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Import libraries<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drew inspiration from https://github.com/dmesquita/understanding_pytorch_nn and\n",
    "#and https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "#and https://github.com/nyu-mll/DS-GA-1011-Fall2017/blob/master/week%20eight/Week%20Eight%20Solutions.ipynb\n",
    "#and https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "#https://github.com/claravania/lstm-pytorch/blob/master/model.py\n",
    "#https://medium.com/@sonicboom8/sentiment-analysis-with-variable-length-sequences-in-pytorch-6241635ae130\n",
    "#https://github.com/hpanwar08/sentence-classification-pytorch/blob/master/Sentiment%20analysis%20pytorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Data Processing<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = ['CAPS', 'Obscenity', 'Threat', 'hatespeech', 'namecalling', 'negprejudice', 'noneng', 'porn', 'stereotypes']\n",
    "\n",
    "for label in labels:\n",
    "    cols = [label + str(x) for x in range(1,8)]\n",
    "    train[label + '_num_yes'] = train[cols].sum(axis = 1)\n",
    "    train[label] = pd.Series(train[label + '_num_yes'] >= 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.loc[train['clean_tweet'].isnull() == False,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CAPS1</th>\n",
       "      <th>CAPS2</th>\n",
       "      <th>CAPS3</th>\n",
       "      <th>CAPS4</th>\n",
       "      <th>CAPS5</th>\n",
       "      <th>CAPS6</th>\n",
       "      <th>CAPS7</th>\n",
       "      <th>Obscenity1</th>\n",
       "      <th>Obscenity2</th>\n",
       "      <th>...</th>\n",
       "      <th>namecalling_num_yes</th>\n",
       "      <th>namecalling</th>\n",
       "      <th>negprejudice_num_yes</th>\n",
       "      <th>negprejudice</th>\n",
       "      <th>noneng_num_yes</th>\n",
       "      <th>noneng</th>\n",
       "      <th>porn_num_yes</th>\n",
       "      <th>porn</th>\n",
       "      <th>stereotypes_num_yes</th>\n",
       "      <th>stereotypes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  CAPS1  CAPS2  CAPS3  CAPS4  CAPS5  CAPS6  CAPS7  Obscenity1  \\\n",
       "0         420    0.0    0.0    0.0    0.0    NaN    NaN    NaN         0.0   \n",
       "1        1676    0.0    NaN    NaN    NaN    0.0    NaN    0.0         0.0   \n",
       "2         742    NaN    NaN    NaN    NaN    0.0    0.0    0.0         NaN   \n",
       "3         791    NaN    NaN    0.0    0.0    NaN    0.0    NaN         NaN   \n",
       "4         164    NaN    NaN    0.0    0.0    NaN    0.0    NaN         NaN   \n",
       "\n",
       "   Obscenity2     ...       namecalling_num_yes  namecalling  \\\n",
       "0         0.0     ...                       0.0            0   \n",
       "1         NaN     ...                       1.0            0   \n",
       "2         NaN     ...                       0.0            0   \n",
       "3         NaN     ...                       0.0            0   \n",
       "4         NaN     ...                       0.0            0   \n",
       "\n",
       "   negprejudice_num_yes  negprejudice  noneng_num_yes  noneng  porn_num_yes  \\\n",
       "0                   0.0             0             0.0       0           0.0   \n",
       "1                   0.0             0             0.0       0           0.0   \n",
       "2                   0.0             0             0.0       0           0.0   \n",
       "3                   0.0             0             0.0       0           0.0   \n",
       "4                   0.0             0             3.0       1           0.0   \n",
       "\n",
       "   porn  stereotypes_num_yes  stereotypes  \n",
       "0     0                  0.0            0  \n",
       "1     0                  0.0            0  \n",
       "2     0                  1.0            0  \n",
       "3     0                  0.0            0  \n",
       "4     0                  0.0            0  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in train.clean_tweet:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "for text in train.clean_tweet:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "total_words = len(vocab)\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i+1\n",
    "\n",
    "    return word2index\n",
    "\n",
    "word2index = get_word_2_index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to make indexer start at 1, because 0 is a pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_data(s, length):\n",
    "    padded = np.zeros((length,), dtype = np.int64)\n",
    "    if len(s) > length: \n",
    "        padded = s[:length]\n",
    "    else:\n",
    "        padded[:len(s)] = s\n",
    "    return np.array(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['seq_len'] = [len(x.split(' ')) for x in train['clean_tweet']]\n",
    "\n",
    "train['numeric'] = [[word2index[y] for y in x.split(' ')] for x in train['clean_tweet']]\n",
    "\n",
    "train['padded_tweet'] = [pad_data(x, 20) for x in train.numeric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subclass the custom dataset class with torch.utils.data.Dataset\n",
    "# implement __len__ and __getitem__ function\n",
    "class VectorizeData(Dataset):\n",
    "    def __init__(self, df, label, maxlen=20):\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.df.padded_tweet[idx]\n",
    "        y = self.df[self.label][idx]\n",
    "        lens = self.df.seq_len[idx]\n",
    "        return X,y,lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = VectorizeData(train, label = 'hatespeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(data, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.dropout_layer = nn.Dropout(p=0.2)\n",
    "        self.batch_size = batch_size\n",
    "        #self.hidden = self.init_hidden(batch_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return(autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)), \\\n",
    "               autograd.Variable(torch.randn(1, batch_size, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, batch): #removed lengths\n",
    "        #should reinitalize hidden states before each batch?\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.embedding(batch)\n",
    "        #packed_input = pack_padded_sequence(embeds, lengths)\n",
    "        outputs, (ht, ct) = self.lstm(embeds, self.hidden)\n",
    "        # ht is the last hidden state of the sequences\n",
    "        # ht = (1 x batch_size x hidden_dim)\n",
    "        # ht[-1] = (batch_size x hidden_dim)\n",
    "        output = self.dropout_layer(ht[-1])\n",
    "        output = self.hidden2out(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 100 \n",
    "num_classes = 2\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4/124], Loss: 0.7770\n",
      "Epoch [1/5], Step [8/124], Loss: 0.6642\n",
      "Epoch [1/5], Step [12/124], Loss: 0.5778\n",
      "Epoch [1/5], Step [16/124], Loss: 0.4910\n",
      "Epoch [1/5], Step [20/124], Loss: 0.4693\n",
      "Epoch [1/5], Step [24/124], Loss: 0.4203\n",
      "Epoch [1/5], Step [28/124], Loss: 0.3052\n",
      "Epoch [1/5], Step [32/124], Loss: 0.2595\n",
      "Epoch [1/5], Step [36/124], Loss: 0.3130\n",
      "Epoch [1/5], Step [40/124], Loss: 0.2492\n",
      "Epoch [1/5], Step [44/124], Loss: 0.1811\n",
      "Epoch [1/5], Step [48/124], Loss: 0.2075\n",
      "Epoch [1/5], Step [52/124], Loss: 0.1843\n",
      "Epoch [1/5], Step [56/124], Loss: 0.3317\n",
      "Epoch [1/5], Step [60/124], Loss: 0.1812\n",
      "Epoch [1/5], Step [64/124], Loss: 0.1469\n",
      "Epoch [1/5], Step [68/124], Loss: 0.0927\n",
      "Epoch [1/5], Step [72/124], Loss: 0.2293\n",
      "Epoch [1/5], Step [76/124], Loss: 0.1484\n",
      "Epoch [1/5], Step [80/124], Loss: 0.1551\n",
      "Epoch [1/5], Step [84/124], Loss: 0.0687\n",
      "Epoch [1/5], Step [88/124], Loss: 0.0501\n",
      "Epoch [1/5], Step [92/124], Loss: 0.0557\n",
      "Epoch [1/5], Step [96/124], Loss: 0.3378\n",
      "Epoch [1/5], Step [100/124], Loss: 0.0399\n",
      "Epoch [1/5], Step [104/124], Loss: 0.2641\n",
      "Epoch [1/5], Step [108/124], Loss: 0.0364\n",
      "Epoch [1/5], Step [112/124], Loss: 0.1450\n",
      "Epoch [1/5], Step [116/124], Loss: 0.1532\n",
      "Epoch [1/5], Step [120/124], Loss: 0.2600\n",
      "Epoch [1/5], Step [124/124], Loss: 0.1380\n",
      "Epoch [2/5], Step [4/124], Loss: 0.0265\n",
      "Epoch [2/5], Step [8/124], Loss: 0.0282\n",
      "Epoch [2/5], Step [12/124], Loss: 0.0256\n",
      "Epoch [2/5], Step [16/124], Loss: 0.0273\n",
      "Epoch [2/5], Step [20/124], Loss: 0.2671\n",
      "Epoch [2/5], Step [24/124], Loss: 0.2720\n",
      "Epoch [2/5], Step [28/124], Loss: 0.0267\n",
      "Epoch [2/5], Step [32/124], Loss: 0.0238\n",
      "Epoch [2/5], Step [36/124], Loss: 0.2493\n",
      "Epoch [2/5], Step [40/124], Loss: 0.1443\n",
      "Epoch [2/5], Step [44/124], Loss: 0.0360\n",
      "Epoch [2/5], Step [48/124], Loss: 0.1524\n",
      "Epoch [2/5], Step [52/124], Loss: 0.1472\n",
      "Epoch [2/5], Step [56/124], Loss: 0.3924\n",
      "Epoch [2/5], Step [60/124], Loss: 0.1437\n",
      "Epoch [2/5], Step [64/124], Loss: 0.1498\n",
      "Epoch [2/5], Step [68/124], Loss: 0.0458\n",
      "Epoch [2/5], Step [72/124], Loss: 0.2903\n",
      "Epoch [2/5], Step [76/124], Loss: 0.1422\n",
      "Epoch [2/5], Step [80/124], Loss: 0.1491\n",
      "Epoch [2/5], Step [84/124], Loss: 0.0440\n",
      "Epoch [2/5], Step [88/124], Loss: 0.0251\n",
      "Epoch [2/5], Step [92/124], Loss: 0.0363\n",
      "Epoch [2/5], Step [96/124], Loss: 0.3686\n",
      "Epoch [2/5], Step [100/124], Loss: 0.0273\n",
      "Epoch [2/5], Step [104/124], Loss: 0.2524\n",
      "Epoch [2/5], Step [108/124], Loss: 0.0237\n",
      "Epoch [2/5], Step [112/124], Loss: 0.1419\n",
      "Epoch [2/5], Step [116/124], Loss: 0.1470\n",
      "Epoch [2/5], Step [120/124], Loss: 0.2554\n",
      "Epoch [2/5], Step [124/124], Loss: 0.1499\n",
      "Epoch [3/5], Step [4/124], Loss: 0.0227\n",
      "Epoch [3/5], Step [8/124], Loss: 0.0236\n",
      "Epoch [3/5], Step [12/124], Loss: 0.0216\n",
      "Epoch [3/5], Step [16/124], Loss: 0.0211\n",
      "Epoch [3/5], Step [20/124], Loss: 0.2801\n",
      "Epoch [3/5], Step [24/124], Loss: 0.2720\n",
      "Epoch [3/5], Step [28/124], Loss: 0.0231\n",
      "Epoch [3/5], Step [32/124], Loss: 0.0238\n",
      "Epoch [3/5], Step [36/124], Loss: 0.2758\n",
      "Epoch [3/5], Step [40/124], Loss: 0.1272\n",
      "Epoch [3/5], Step [44/124], Loss: 0.0340\n",
      "Epoch [3/5], Step [48/124], Loss: 0.1334\n",
      "Epoch [3/5], Step [52/124], Loss: 0.1468\n",
      "Epoch [3/5], Step [56/124], Loss: 0.3679\n",
      "Epoch [3/5], Step [60/124], Loss: 0.1529\n",
      "Epoch [3/5], Step [64/124], Loss: 0.1405\n",
      "Epoch [3/5], Step [68/124], Loss: 0.0391\n",
      "Epoch [3/5], Step [72/124], Loss: 0.2676\n",
      "Epoch [3/5], Step [76/124], Loss: 0.1602\n",
      "Epoch [3/5], Step [80/124], Loss: 0.1632\n",
      "Epoch [3/5], Step [84/124], Loss: 0.0418\n",
      "Epoch [3/5], Step [88/124], Loss: 0.0265\n",
      "Epoch [3/5], Step [92/124], Loss: 0.0373\n",
      "Epoch [3/5], Step [96/124], Loss: 0.3740\n",
      "Epoch [3/5], Step [100/124], Loss: 0.0272\n",
      "Epoch [3/5], Step [104/124], Loss: 0.2658\n",
      "Epoch [3/5], Step [108/124], Loss: 0.0231\n",
      "Epoch [3/5], Step [112/124], Loss: 0.1522\n",
      "Epoch [3/5], Step [116/124], Loss: 0.1420\n",
      "Epoch [3/5], Step [120/124], Loss: 0.2481\n",
      "Epoch [3/5], Step [124/124], Loss: 0.1389\n",
      "Epoch [4/5], Step [4/124], Loss: 0.0225\n",
      "Epoch [4/5], Step [8/124], Loss: 0.0216\n",
      "Epoch [4/5], Step [12/124], Loss: 0.0253\n",
      "Epoch [4/5], Step [16/124], Loss: 0.0227\n",
      "Epoch [4/5], Step [20/124], Loss: 0.2544\n",
      "Epoch [4/5], Step [24/124], Loss: 0.2780\n",
      "Epoch [4/5], Step [28/124], Loss: 0.0207\n",
      "Epoch [4/5], Step [32/124], Loss: 0.0217\n",
      "Epoch [4/5], Step [36/124], Loss: 0.2466\n",
      "Epoch [4/5], Step [40/124], Loss: 0.1328\n",
      "Epoch [4/5], Step [44/124], Loss: 0.0315\n",
      "Epoch [4/5], Step [48/124], Loss: 0.1578\n",
      "Epoch [4/5], Step [52/124], Loss: 0.1523\n",
      "Epoch [4/5], Step [56/124], Loss: 0.3886\n",
      "Epoch [4/5], Step [60/124], Loss: 0.1431\n",
      "Epoch [4/5], Step [64/124], Loss: 0.1215\n",
      "Epoch [4/5], Step [68/124], Loss: 0.0424\n",
      "Epoch [4/5], Step [72/124], Loss: 0.2369\n",
      "Epoch [4/5], Step [76/124], Loss: 0.1611\n",
      "Epoch [4/5], Step [80/124], Loss: 0.1390\n",
      "Epoch [4/5], Step [84/124], Loss: 0.0389\n",
      "Epoch [4/5], Step [88/124], Loss: 0.0235\n",
      "Epoch [4/5], Step [92/124], Loss: 0.0339\n",
      "Epoch [4/5], Step [96/124], Loss: 0.4153\n",
      "Epoch [4/5], Step [100/124], Loss: 0.0245\n",
      "Epoch [4/5], Step [104/124], Loss: 0.2616\n",
      "Epoch [4/5], Step [108/124], Loss: 0.0224\n",
      "Epoch [4/5], Step [112/124], Loss: 0.1709\n",
      "Epoch [4/5], Step [116/124], Loss: 0.1387\n",
      "Epoch [4/5], Step [120/124], Loss: 0.2736\n",
      "Epoch [4/5], Step [124/124], Loss: 0.1342\n",
      "Epoch [5/5], Step [4/124], Loss: 0.0233\n",
      "Epoch [5/5], Step [8/124], Loss: 0.0232\n",
      "Epoch [5/5], Step [12/124], Loss: 0.0217\n",
      "Epoch [5/5], Step [16/124], Loss: 0.0215\n",
      "Epoch [5/5], Step [20/124], Loss: 0.2909\n",
      "Epoch [5/5], Step [24/124], Loss: 0.2864\n",
      "Epoch [5/5], Step [28/124], Loss: 0.0217\n",
      "Epoch [5/5], Step [32/124], Loss: 0.0220\n",
      "Epoch [5/5], Step [36/124], Loss: 0.2655\n",
      "Epoch [5/5], Step [40/124], Loss: 0.1445\n",
      "Epoch [5/5], Step [44/124], Loss: 0.0312\n",
      "Epoch [5/5], Step [48/124], Loss: 0.1335\n",
      "Epoch [5/5], Step [52/124], Loss: 0.1510\n",
      "Epoch [5/5], Step [56/124], Loss: 0.3877\n",
      "Epoch [5/5], Step [60/124], Loss: 0.1378\n",
      "Epoch [5/5], Step [64/124], Loss: 0.1386\n",
      "Epoch [5/5], Step [68/124], Loss: 0.0410\n",
      "Epoch [5/5], Step [72/124], Loss: 0.2539\n",
      "Epoch [5/5], Step [76/124], Loss: 0.1442\n",
      "Epoch [5/5], Step [80/124], Loss: 0.1447\n",
      "Epoch [5/5], Step [84/124], Loss: 0.0403\n",
      "Epoch [5/5], Step [88/124], Loss: 0.0274\n",
      "Epoch [5/5], Step [92/124], Loss: 0.0348\n",
      "Epoch [5/5], Step [96/124], Loss: 0.3620\n",
      "Epoch [5/5], Step [100/124], Loss: 0.0252\n",
      "Epoch [5/5], Step [104/124], Loss: 0.2707\n",
      "Epoch [5/5], Step [108/124], Loss: 0.0224\n",
      "Epoch [5/5], Step [112/124], Loss: 0.1466\n",
      "Epoch [5/5], Step [116/124], Loss: 0.1456\n",
      "Epoch [5/5], Step [120/124], Loss: 0.2417\n",
      "Epoch [5/5], Step [124/124], Loss: 0.1364\n"
     ]
    }
   ],
   "source": [
    "net = LSTMClassifier(total_words, hidden_size, hidden_size, num_classes, batch_size)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    it = iter(dl)\n",
    "    total_batch = int(len(train.clean_tweet)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y,batch_len = next(it)\n",
    "        tweets = Variable(batch_x.transpose(0,1))\n",
    "        labels = Variable(batch_y)\n",
    "        lengths = Variable(batch_len)\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(tweets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(train.clean_tweet)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "it = iter(dl)\n",
    "batch_x,batch_y,batch_len = next(it)\n",
    "tweets = Variable(batch_x.transpose(0,1))\n",
    "labels = Variable(batch_y)\n",
    "lengths = Variable(batch_len)\n",
    "outputs = net(tweets)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total += labels.size(0)\n",
    "correct = (predicted == labels.data).sum()\n",
    "print (correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
