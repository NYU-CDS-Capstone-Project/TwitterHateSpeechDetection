{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Import libraries<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drew inspiration from https://github.com/dmesquita/understanding_pytorch_nn and\n",
    "#and https://github.com/msahamed/yelp_comments_classification_nlp/blob/master/word_embeddings.ipynb\n",
    "#and https://github.com/nyu-mll/DS-GA-1011-Fall2017/blob/master/week%20eight/Week%20Eight%20Solutions.ipynb\n",
    "#and https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "#https://github.com/claravania/lstm-pytorch/blob/master/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Data Processing<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['CAPS', 'Obscenity', 'Threat', 'hatespeech', 'namecalling', 'negprejudice', 'noneng', 'porn', 'stereotypes']\n",
    "\n",
    "for label in labels:\n",
    "    cols = [label + str(x) for x in range(1,8)]\n",
    "    train[label + '_num_yes'] = train[cols].sum(axis = 1)\n",
    "    train[label] = train[label + '_num_yes'] >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.loc[train['clean_tweet'].isnull() == False,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in train.clean_tweet:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "for text in train.clean_tweet:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word.lower()]+=1\n",
    "\n",
    "total_words = len(vocab)\n",
    "\n",
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word.lower()] = i\n",
    "\n",
    "    return word2index\n",
    "\n",
    "word2index = get_word_2_index(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['seq_len'] = [len(x.split(' ')) for x in train['clean_tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(df,i,batch_size,x_name,target_name,length_name):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df[x_name][i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df[target_name][i*batch_size:i*batch_size+batch_size]\n",
    "    lengths = df[length_name][i*batch_size:i*batch_size+batch_size]\n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "        for word in text.split(' '):\n",
    "            layer[word2index[word.lower()]] += 1\n",
    "\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        index_y = -1\n",
    "        if category == 0:\n",
    "            index_y = 0\n",
    "        elif category == 1:\n",
    "            index_y = 1\n",
    "        else:\n",
    "            index_y = 2\n",
    "        results.append(index_y)\n",
    "\n",
    "\n",
    "    return np.array(batches),np.array(results),np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OurNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(OurNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_size,hidden_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 100 \n",
    "num_classes=2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4/124], Loss: 0.6760\n",
      "Epoch [1/5], Step [8/124], Loss: 0.6338\n",
      "Epoch [1/5], Step [12/124], Loss: 0.5884\n",
      "Epoch [1/5], Step [16/124], Loss: 0.5210\n",
      "Epoch [1/5], Step [20/124], Loss: 0.5078\n",
      "Epoch [1/5], Step [24/124], Loss: 0.4425\n",
      "Epoch [1/5], Step [28/124], Loss: 0.3274\n",
      "Epoch [1/5], Step [32/124], Loss: 0.2328\n",
      "Epoch [1/5], Step [36/124], Loss: 0.2718\n",
      "Epoch [1/5], Step [40/124], Loss: 0.1794\n",
      "Epoch [1/5], Step [44/124], Loss: 0.0771\n",
      "Epoch [1/5], Step [48/124], Loss: 0.1039\n",
      "Epoch [1/5], Step [52/124], Loss: 0.1014\n",
      "Epoch [1/5], Step [56/124], Loss: 0.3253\n",
      "Epoch [1/5], Step [60/124], Loss: 0.1622\n",
      "Epoch [1/5], Step [64/124], Loss: 0.0638\n",
      "Epoch [1/5], Step [68/124], Loss: 0.0178\n",
      "Epoch [1/5], Step [72/124], Loss: 0.2167\n",
      "Epoch [1/5], Step [76/124], Loss: 0.0536\n",
      "Epoch [1/5], Step [80/124], Loss: 0.0884\n",
      "Epoch [1/5], Step [84/124], Loss: 0.0166\n",
      "Epoch [1/5], Step [88/124], Loss: 0.0264\n",
      "Epoch [1/5], Step [92/124], Loss: 0.0253\n",
      "Epoch [1/5], Step [96/124], Loss: 0.1968\n",
      "Epoch [1/5], Step [100/124], Loss: 0.0268\n",
      "Epoch [1/5], Step [104/124], Loss: 0.1371\n",
      "Epoch [1/5], Step [108/124], Loss: 0.0180\n",
      "Epoch [1/5], Step [112/124], Loss: 0.0476\n",
      "Epoch [1/5], Step [116/124], Loss: 0.0555\n",
      "Epoch [1/5], Step [120/124], Loss: 0.2763\n",
      "Epoch [1/5], Step [124/124], Loss: 0.0534\n",
      "Epoch [2/5], Step [4/124], Loss: 0.0117\n",
      "Epoch [2/5], Step [8/124], Loss: 0.0126\n",
      "Epoch [2/5], Step [12/124], Loss: 0.0213\n",
      "Epoch [2/5], Step [16/124], Loss: 0.0067\n",
      "Epoch [2/5], Step [20/124], Loss: 0.1700\n",
      "Epoch [2/5], Step [24/124], Loss: 0.0721\n",
      "Epoch [2/5], Step [28/124], Loss: 0.0152\n",
      "Epoch [2/5], Step [32/124], Loss: 0.0080\n",
      "Epoch [2/5], Step [36/124], Loss: 0.0917\n",
      "Epoch [2/5], Step [40/124], Loss: 0.0875\n",
      "Epoch [2/5], Step [44/124], Loss: 0.0124\n",
      "Epoch [2/5], Step [48/124], Loss: 0.0408\n",
      "Epoch [2/5], Step [52/124], Loss: 0.0330\n",
      "Epoch [2/5], Step [56/124], Loss: 0.1242\n",
      "Epoch [2/5], Step [60/124], Loss: 0.1212\n",
      "Epoch [2/5], Step [64/124], Loss: 0.0297\n",
      "Epoch [2/5], Step [68/124], Loss: 0.0128\n",
      "Epoch [2/5], Step [72/124], Loss: 0.1240\n",
      "Epoch [2/5], Step [76/124], Loss: 0.0287\n",
      "Epoch [2/5], Step [80/124], Loss: 0.0307\n",
      "Epoch [2/5], Step [84/124], Loss: 0.0065\n",
      "Epoch [2/5], Step [88/124], Loss: 0.0070\n",
      "Epoch [2/5], Step [92/124], Loss: 0.0137\n",
      "Epoch [2/5], Step [96/124], Loss: 0.0671\n",
      "Epoch [2/5], Step [100/124], Loss: 0.0096\n",
      "Epoch [2/5], Step [104/124], Loss: 0.0946\n",
      "Epoch [2/5], Step [108/124], Loss: 0.0078\n",
      "Epoch [2/5], Step [112/124], Loss: 0.0251\n",
      "Epoch [2/5], Step [116/124], Loss: 0.0218\n",
      "Epoch [2/5], Step [120/124], Loss: 0.0709\n",
      "Epoch [2/5], Step [124/124], Loss: 0.0262\n",
      "Epoch [3/5], Step [4/124], Loss: 0.0040\n",
      "Epoch [3/5], Step [8/124], Loss: 0.0045\n",
      "Epoch [3/5], Step [12/124], Loss: 0.0178\n",
      "Epoch [3/5], Step [16/124], Loss: 0.0029\n",
      "Epoch [3/5], Step [20/124], Loss: 0.0655\n",
      "Epoch [3/5], Step [24/124], Loss: 0.0465\n",
      "Epoch [3/5], Step [28/124], Loss: 0.0066\n",
      "Epoch [3/5], Step [32/124], Loss: 0.0043\n",
      "Epoch [3/5], Step [36/124], Loss: 0.0315\n",
      "Epoch [3/5], Step [40/124], Loss: 0.0540\n",
      "Epoch [3/5], Step [44/124], Loss: 0.0031\n",
      "Epoch [3/5], Step [48/124], Loss: 0.0157\n",
      "Epoch [3/5], Step [52/124], Loss: 0.0124\n",
      "Epoch [3/5], Step [56/124], Loss: 0.0722\n",
      "Epoch [3/5], Step [60/124], Loss: 0.0841\n",
      "Epoch [3/5], Step [64/124], Loss: 0.0059\n",
      "Epoch [3/5], Step [68/124], Loss: 0.0041\n",
      "Epoch [3/5], Step [72/124], Loss: 0.0476\n",
      "Epoch [3/5], Step [76/124], Loss: 0.0090\n",
      "Epoch [3/5], Step [80/124], Loss: 0.0071\n",
      "Epoch [3/5], Step [84/124], Loss: 0.0017\n",
      "Epoch [3/5], Step [88/124], Loss: 0.0026\n",
      "Epoch [3/5], Step [92/124], Loss: 0.0063\n",
      "Epoch [3/5], Step [96/124], Loss: 0.0109\n",
      "Epoch [3/5], Step [100/124], Loss: 0.0033\n",
      "Epoch [3/5], Step [104/124], Loss: 0.0280\n",
      "Epoch [3/5], Step [108/124], Loss: 0.0034\n",
      "Epoch [3/5], Step [112/124], Loss: 0.0055\n",
      "Epoch [3/5], Step [116/124], Loss: 0.0016\n",
      "Epoch [3/5], Step [120/124], Loss: 0.0207\n",
      "Epoch [3/5], Step [124/124], Loss: 0.0046\n",
      "Epoch [4/5], Step [4/124], Loss: 0.0008\n",
      "Epoch [4/5], Step [8/124], Loss: 0.0011\n",
      "Epoch [4/5], Step [12/124], Loss: 0.0453\n",
      "Epoch [4/5], Step [16/124], Loss: 0.0004\n",
      "Epoch [4/5], Step [20/124], Loss: 0.0083\n",
      "Epoch [4/5], Step [24/124], Loss: 0.0215\n",
      "Epoch [4/5], Step [28/124], Loss: 0.0013\n",
      "Epoch [4/5], Step [32/124], Loss: 0.0049\n",
      "Epoch [4/5], Step [36/124], Loss: 0.0038\n",
      "Epoch [4/5], Step [40/124], Loss: 0.0150\n",
      "Epoch [4/5], Step [44/124], Loss: 0.0007\n",
      "Epoch [4/5], Step [48/124], Loss: 0.0024\n",
      "Epoch [4/5], Step [52/124], Loss: 0.0014\n",
      "Epoch [4/5], Step [56/124], Loss: 0.0450\n",
      "Epoch [4/5], Step [60/124], Loss: 0.0386\n",
      "Epoch [4/5], Step [64/124], Loss: 0.0030\n",
      "Epoch [4/5], Step [68/124], Loss: 0.0011\n",
      "Epoch [4/5], Step [72/124], Loss: 0.0079\n",
      "Epoch [4/5], Step [76/124], Loss: 0.0019\n",
      "Epoch [4/5], Step [80/124], Loss: 0.0009\n",
      "Epoch [4/5], Step [84/124], Loss: 0.0003\n",
      "Epoch [4/5], Step [88/124], Loss: 0.0006\n",
      "Epoch [4/5], Step [92/124], Loss: 0.0015\n",
      "Epoch [4/5], Step [96/124], Loss: 0.0016\n",
      "Epoch [4/5], Step [100/124], Loss: 0.0008\n",
      "Epoch [4/5], Step [104/124], Loss: 0.0068\n",
      "Epoch [4/5], Step [108/124], Loss: 0.0010\n",
      "Epoch [4/5], Step [112/124], Loss: 0.0020\n",
      "Epoch [4/5], Step [116/124], Loss: 0.0002\n",
      "Epoch [4/5], Step [120/124], Loss: 0.0063\n",
      "Epoch [4/5], Step [124/124], Loss: 0.0012\n",
      "Epoch [5/5], Step [4/124], Loss: 0.0002\n",
      "Epoch [5/5], Step [8/124], Loss: 0.0003\n",
      "Epoch [5/5], Step [12/124], Loss: 0.0534\n",
      "Epoch [5/5], Step [16/124], Loss: 0.0001\n",
      "Epoch [5/5], Step [20/124], Loss: 0.0028\n",
      "Epoch [5/5], Step [24/124], Loss: 0.0210\n",
      "Epoch [5/5], Step [28/124], Loss: 0.0005\n",
      "Epoch [5/5], Step [32/124], Loss: 0.0041\n",
      "Epoch [5/5], Step [36/124], Loss: 0.0015\n",
      "Epoch [5/5], Step [40/124], Loss: 0.0083\n",
      "Epoch [5/5], Step [44/124], Loss: 0.0003\n",
      "Epoch [5/5], Step [48/124], Loss: 0.0010\n",
      "Epoch [5/5], Step [52/124], Loss: 0.0005\n",
      "Epoch [5/5], Step [56/124], Loss: 0.0448\n",
      "Epoch [5/5], Step [60/124], Loss: 0.0153\n",
      "Epoch [5/5], Step [64/124], Loss: 0.0015\n",
      "Epoch [5/5], Step [68/124], Loss: 0.0004\n",
      "Epoch [5/5], Step [72/124], Loss: 0.0048\n",
      "Epoch [5/5], Step [76/124], Loss: 0.0009\n",
      "Epoch [5/5], Step [80/124], Loss: 0.0003\n",
      "Epoch [5/5], Step [84/124], Loss: 0.0001\n",
      "Epoch [5/5], Step [88/124], Loss: 0.0003\n",
      "Epoch [5/5], Step [92/124], Loss: 0.0007\n",
      "Epoch [5/5], Step [96/124], Loss: 0.0006\n",
      "Epoch [5/5], Step [100/124], Loss: 0.0004\n",
      "Epoch [5/5], Step [104/124], Loss: 0.0045\n",
      "Epoch [5/5], Step [108/124], Loss: 0.0005\n",
      "Epoch [5/5], Step [112/124], Loss: 0.0011\n",
      "Epoch [5/5], Step [116/124], Loss: 0.0001\n",
      "Epoch [5/5], Step [120/124], Loss: 0.0026\n",
      "Epoch [5/5], Step [124/124], Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "net = OurNet(total_words, hidden_size,num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    total_batch = int(len(train.clean_tweet)/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_x,batch_y,batch_len = get_batch(train,i,batch_size,'clean_tweet','hatespeech','seq_len')\n",
    "        tweets = Variable(torch.FloatTensor(batch_x))\n",
    "        labels = Variable(torch.LongTensor(batch_y))\n",
    "        lengths = Variable(torch.LongTensor(batch_len))\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(tweets)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                   %(epoch+1, num_epochs, i+1, len(train.clean_tweet)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.996\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "batch_x_test,batch_y_test,batch_x_len = get_batch(train,0,500,'clean_tweet','hatespeech','seq_len')\n",
    "articles = Variable(torch.FloatTensor(batch_x_test))\n",
    "labels = torch.LongTensor(batch_y_test)\n",
    "outputs = net(articles)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "total += labels.size(0)\n",
    "correct += (predicted == labels).sum()\n",
    "print (correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#99.6% accuracy on hatespeech? only caveat is, it may be overfit b/c didn't use a holdout set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
